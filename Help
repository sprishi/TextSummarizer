import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# Assuming df is your DataFrame

# 2. Encoding the Target Variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df['loadcountry'])

# 3. Train-Test Split
features = ['no', 'dep_month', 'arr_month', 'prev_load_ctr', 'prev_disc_ctr', 'prev_load_ctr1', 'prev_disc_ctr1', 'prev_load_ctr2', 'prev_disc_ctr2', 'mean_voyage_wk_routes', 'route_class_count', 'unique_routes', 'datagroup_1', 'datagroup_2']
X = df[features]
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.07, random_state=42)

# 4. Model Training
model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='mlogloss')
model.fit(X_train, y_train)

# 5. Model Evaluation using predict
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.2f}")
print(f"F1 Score: {f1:.2f}")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# 6. Top 3 Predictions
probabilities = model.predict_proba(X_test)
top3_indices = np.argsort(-probabilities, axis=1)[:, :3]
top3_labels = [label_encoder.inverse_transform([idx])[0] for idx in top3_indices.flatten()]
top3_labels = np.array(top3_labels).reshape(-1, 3)

top3_probs = np.array([probabilities[i, top3_indices[i]] for i in range(probabilities.shape[0])]) * 100  # Convert to percentages

results_df = pd.DataFrame({
    'Flight_No': X_test['no'].values,
    'Predicted_Load_Country': y_pred,  # primary prediction using predict method
    '1st_Predicted_Load_Country': top3_labels[:, 0],
    '1st_Prediction_Probability_%': top3_probs[:, 0],
    '2nd_Predicted_Load_Country': top3_labels[:, 1],
    '2nd_Prediction_Probability_%': top3_probs[:, 1],
    '3rd_Predicted_Load_Country': top3_labels[:, 2],
    '3rd_Prediction_Probability_%': top3_probs[:, 2]
})

print(results_df.head())



---------------------------------




import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# Assuming df is your DataFrame and is sorted chronologically.

# Encoding the Target Variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df['loadcountry'])

# Sequential Train-Test Split: Last occurrence of each flight for test
test_indices = df.groupby('no').tail(1).index
X_test = df.loc[test_indices, features]
y_test = y_encoded[test_indices]

train_indices = df.index.difference(test_indices)
X_train = df.loc[train_indices, features]
y_train = y_encoded[train_indices]

# Hyperparameter Grid
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'max_depth': [3, 4, 5, 6, 7, 8],
    'n_estimators': [100, 500, 1000],
    'gamma': [0, 0.1, 0.5],
    'subsample': [1.0],  # set to 1.0 to avoid subsampling
    'colsample_bytree': [0.6, 0.8, 1.0],
    'min_child_weight': [1, 5, 10]
}

# Randomized Search
xgb_model = xgb.XGBClassifier(objective='multi:softprob', random_state=42, eval_metric='mlogloss')
random_search = RandomizedSearchCV(xgb_model, param_distributions=param_grid, n_iter=50, scoring='f1_macro', n_jobs=-1, cv=3, verbose=3, random_state=42)
random_search.fit(X_train, y_train)

# Model Training with Best Parameters
best_model = random_search.best_estimator_
best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)

# Model Evaluation
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='macro')

print(f"Accuracy: {accuracy:.2f}")
print(f"F1 Score (Macro): {f1:.2f}")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))


