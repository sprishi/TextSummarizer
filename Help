import pandas as pd
import numpy as np
from scipy.spatial import KDTree
from geopy.distance import geodesic

# Convert latitude and longitude to Cartesian coordinates for KDTree
def lat_lon_to_cartesian(lat, lon, R = 6371):
    lat, lon = np.radians(lat), np.radians(lon)
    x = R * np.cos(lat) * np.cos(lon)
    y = R * np.cos(lat) * np.sin(lon)
    z = R * np.sin(lat)
    return x, y, z

# Function to find the nearest location using KDTree
def find_nearest_location(person_df, location_df):
    # Convert location data to Cartesian coordinates
    loc_coords = np.array([lat_lon_to_cartesian(lat, lon) for lat, lon in zip(location_df['latitude'], location_df['longitude'])])
    
    # Build KDTree using location coordinates
    tree = KDTree(loc_coords)

    nearest_locations = []
    
    for _, person in person_df.iterrows():
        person_coord = lat_lon_to_cartesian(person['latitude'], person['longitude'])

        # Query the KDTree to find the nearest location
        dist, index = tree.query(person_coord)
        
        nearest_location = location_df.iloc[index]['location_name']
        nearest_coord = location_df.iloc[index][['latitude', 'longitude']]
        
        # Convert distance from Cartesian to actual (nautical miles)
        actual_dist = geodesic((person['latitude'], person['longitude']), (nearest_coord['latitude'], nearest_coord['longitude'])).nautical
        
        nearest_locations.append((nearest_location, actual_dist))

    return nearest_locations

# Example usage with your dataframes: person_df and location_df

# nearest_location_results = find_nearest_location(person_df, location_df)
# person_df[['nearest_location', 'distance_nm']] = pd.DataFrame(nearest_location_results, index=person_df.index)

# print(person_df)





==============

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# Assuming df is your DataFrame

# 2. Encoding the Target Variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df['loadcountry'])

# 3. Train-Test Split
features = ['no', 'dep_month', 'arr_month', 'prev_load_ctr', 'prev_disc_ctr', 'prev_load_ctr1', 'prev_disc_ctr1', 'prev_load_ctr2', 'prev_disc_ctr2', 'mean_voyage_wk_routes', 'route_class_count', 'unique_routes', 'datagroup_1', 'datagroup_2']
X = df[features]
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.07, random_state=42)

# 4. Model Training
model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='mlogloss')
model.fit(X_train, y_train)

# 5. Model Evaluation using predict
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.2f}")
print(f"F1 Score: {f1:.2f}")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# 6. Top 3 Predictions
probabilities = model.predict_proba(X_test)
top3_indices = np.argsort(-probabilities, axis=1)[:, :3]
top3_labels = [label_encoder.inverse_transform([idx])[0] for idx in top3_indices.flatten()]
top3_labels = np.array(top3_labels).reshape(-1, 3)

top3_probs = np.array([probabilities[i, top3_indices[i]] for i in range(probabilities.shape[0])]) * 100  # Convert to percentages

results_df = pd.DataFrame({
    'Flight_No': X_test['no'].values,
    'Predicted_Load_Country': y_pred,  # primary prediction using predict method
    '1st_Predicted_Load_Country': top3_labels[:, 0],
    '1st_Prediction_Probability_%': top3_probs[:, 0],
    '2nd_Predicted_Load_Country': top3_labels[:, 1],
    '2nd_Prediction_Probability_%': top3_probs[:, 1],
    '3rd_Predicted_Load_Country': top3_labels[:, 2],
    '3rd_Prediction_Probability_%': top3_probs[:, 2]
})

print(results_df.head())



---------------------------------




import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# Assuming df is your DataFrame and is sorted chronologically.

# Encoding the Target Variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(df['loadcountry'])

# Sequential Train-Test Split: Last occurrence of each flight for test
test_indices = df.groupby('no').tail(1).index
X_test = df.loc[test_indices, features]
y_test = y_encoded[test_indices]

train_indices = df.index.difference(test_indices)
X_train = df.loc[train_indices, features]
y_train = y_encoded[train_indices]

# Hyperparameter Grid
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'max_depth': [3, 4, 5, 6, 7, 8],
    'n_estimators': [100, 500, 1000],
    'gamma': [0, 0.1, 0.5],
    'subsample': [1.0],  # set to 1.0 to avoid subsampling
    'colsample_bytree': [0.6, 0.8, 1.0],
    'min_child_weight': [1, 5, 10]
}

# Randomized Search
xgb_model = xgb.XGBClassifier(objective='multi:softprob', random_state=42, eval_metric='mlogloss')
random_search = RandomizedSearchCV(xgb_model, param_distributions=param_grid, n_iter=50, scoring='f1_macro', n_jobs=-1, cv=3, verbose=3, random_state=42)
random_search.fit(X_train, y_train)

# Model Training with Best Parameters
best_model = random_search.best_estimator_
best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)

# Model Evaluation
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='macro')

print(f"Accuracy: {accuracy:.2f}")
print(f"F1 Score (Macro): {f1:.2f}")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))


