
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

# ==========================
# Load and Preprocess Data
# ==========================
# Load dataset
data = pd.read_csv('your_dataset.csv', parse_dates=['date'])
data = data.sort_values('date')

# Feature Engineering
data['td_rate_lag1'] = data['td_rate'].shift(1)
data['td_rate_lag7'] = data['td_rate'].shift(7)
data['td_rate_lag30'] = data['td_rate'].shift(30)
data['rolling_mean'] = data['td_rate'].rolling(window=20).mean()
data['rolling_std'] = data['td_rate'].rolling(window=20).std()
data['price_outlier'] = ((data['td_rate'] - data['rolling_mean']) > 2.25 * data['rolling_std']).astype(int)
data['price_outlier'] = data['price_outlier'].shift(-1).fillna(0)
data = data.dropna()

# Features and Target
features = ['td_rate_lag1', 'td_rate_lag7', 'td_rate_lag30', 'price_outlier', 'rolling_mean', 'rolling_std']
daily_target = data['td_rate']
daily_features = data[features]

# Split Data into Train, Validation, and Test Sets
X_train, X_temp, y_train, y_temp = train_test_split(daily_features, daily_target, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize Data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Reshape for LSTM
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)
X_test_lstm = X_test_tensor.unsqueeze(1)

# ==========================
# Define LSTM Model
# ==========================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])
        return output

input_dim = len(features)
hidden_dim = 64
output_dim = 1
model = LSTMModel(input_dim, hidden_dim, output_dim)

# Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================
# Train the Model
# ==========================
epochs = 50
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()

    # Validation loss
    model.eval()
    with torch.no_grad():
        val_output = model(X_val_lstm)
        val_loss = criterion(val_output, y_val_tensor)

    if epoch % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

# ==========================
# Evaluate the Model
# ==========================
model.eval()
with torch.no_grad():
    test_output = model(X_test_lstm).numpy()

# Calculate Metrics
mae = mean_absolute_error(y_test, test_output)
rmse = mean_squared_error(y_test, test_output, squared=False)
print(f"Test MAE: {mae:.2f}, Test RMSE: {rmse:.2f}")

# Calculate Directional Accuracy
actual_directions = np.sign(np.diff(y_test.values))
predicted_directions = np.sign(np.diff(test_output.flatten()))
directional_accuracy = (actual_directions == predicted_directions).mean() * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

# ==========================
# Create Evaluation Table
# ==========================
test_dates = data['date'].iloc[-len(test_output):]  # Match test set length
eval_table = pd.DataFrame({
    'Date': test_dates,
    'Actual Value': y_test.values,
    'Predicted Value': test_output.flatten(),
    'Predicted Lower': np.percentile(test_output, 2.5),
    'Predicted Upper': np.percentile(test_output, 97.5),
    'Predicted Direction': np.concatenate(([0], predicted_directions))  # Add a dummy value for alignment
})
print(eval_table)

# ==========================
# Monthly Predictions
# ==========================
data['month'] = data['date'].dt.to_period('M')
monthly_data = data.groupby('month').agg({
    'td_rate': 'mean',
    'td_rate_lag1': 'mean',
    'td_rate_lag7': 'mean',
    'td_rate_lag30': 'mean',
    'price_outlier': 'max'
}).reset_index()

monthly_features = ['td_rate_lag1', 'td_rate_lag7', 'td_rate_lag30', 'price_outlier']
X_monthly = scaler.fit_transform(monthly_data[monthly_features])
y_monthly = monthly_data['td_rate']

# Train-Test Split for Monthly Data
train_monthly_size = int(len(monthly_data) * 0.8)
X_monthly_train = X_monthly[:train_monthly_size]
y_monthly_train = y_monthly[:train_monthly_size]
X_monthly_test = X_monthly[train_monthly_size:]
y_monthly_test = y_monthly[train_monthly_size:]

# Train Random Forest for Monthly Predictions
monthly_model = RandomForestRegressor(n_estimators=100, random_state=42)
monthly_model.fit(X_monthly_train, y_monthly_train)

# Evaluate Monthly Predictions
monthly_predictions = monthly_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly MAE: {mae_monthly:.2f}")

# Monthly Evaluation Table
monthly_eval_table = pd.DataFrame({
    'Month': monthly_data['month'].iloc[train_monthly_size:].reset_index(drop=True),
    'Actual Value': y_monthly_test,
    'Predicted Value': monthly_predictions
})
print(monthly_eval_table)


#==========

# Import required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load data (replace with actual file paths)
ais_data = pd.read_csv("ais_data.csv")
crude_prices = pd.read_csv("crude_prices.csv")
market_data = pd.read_csv("market_data.csv")
supply_demand = pd.read_csv("supply_demand.csv")

# Merge datasets on the 'date' column
data = pd.merge(ais_data, crude_prices, on="date")
data = pd.merge(data, market_data, on="date")
data = pd.merge(data, supply_demand, on="date")

# Convert 'date' to datetime
data['date'] = pd.to_datetime(data['date'])

# Create month-year column for aggregation purposes
data['month'] = data['date'].dt.to_period('M')

# ==========================
# Feature Engineering
# ==========================

# Lag features
data['brent_price_lag1'] = data['brent_price'].shift(1)
data['td_rate_lag1'] = data['td_rate'].shift(1)  # Include today's price to predict tomorrow
data['laden_perc_7day_avg'] = data['laden_perc'].rolling(window=7).mean()

# Cumulative and scenario features
data['cum_count_vlcc'] = data.groupby('month')['Count_VLCC'].cumsum()
data['Exp_TD3C_KBD_scenario'] = data['Exp_TD3C_KBD'] + 500  # Example scenario

# Dummy variable for global events
std_dev_threshold = 2.25
window_size = 20
rolling_mean = data['td_rate'].rolling(window=window_size).mean()
rolling_std = data['td_rate'].rolling(window=window_size).std()
data['price_outlier'] = ((data['td_rate'] - rolling_mean) > std_dev_threshold * rolling_std).astype(int)

# Shift dummy variable to account for impact on the next day
data['price_outlier'] = data['price_outlier'].shift(-1).fillna(0)

# Add an improved dummy variable (1: up, -1: down, 0: stable)
data['price_movement'] = 0
data.loc[data['td_rate'].diff() > 0, 'price_movement'] = 1
data.loc[data['td_rate'].diff() < 0, 'price_movement'] = -1

# Drop rows with NaN after feature creation
data = data.dropna()

# ==========================
# Train-Validation-Test Split
# ==========================
# Split into train, validation, and test sets
train_data = data[data['date'] < '2024-01-01']
val_data = data[(data['date'] >= '2024-01-01') & (data['date'] < '2024-07-01')]
test_data = data[data['date'] >= '2024-07-01']

X_train = train_data.drop(columns=['td_rate', 'date', 'month'])
y_train = train_data['td_rate']
X_val = val_data.drop(columns=['td_rate', 'date', 'month'])
y_val = val_data['td_rate']
X_test = test_data.drop(columns=['td_rate', 'date', 'month'])
y_test = test_data['td_rate']

# Normalize data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert to tensors for PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Reshape input for LSTM (3D input: [samples, timesteps, features])
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)
X_test_lstm = X_test_tensor.unsqueeze(1)

# ==========================
# Define LSTM Model
# ==========================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last time step output
        return output

# Initialize model, loss, and optimizer
input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = 1

model = LSTMModel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================
# Train the LSTM Model
# ==========================
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        val_output = model(X_val_lstm)
        val_loss = criterion(val_output, y_val_tensor)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

# ==========================
# Evaluate Daily Prediction
# ==========================
model.eval()
with torch.no_grad():
    daily_predictions = model(X_test_lstm).numpy()

# Calculate MAE
mae_daily = mean_absolute_error(y_test, daily_predictions)
print(f"Daily Prediction MAE: {mae_daily:.2f}")

# Calculate Directional Accuracy
y_test_direction = np.sign(y_test.diff().fillna(0))
daily_predictions_direction = np.sign(pd.Series(daily_predictions.flatten()).diff().fillna(0))
directional_accuracy = (y_test_direction == daily_predictions_direction).mean() * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

# Calculate Prediction Range
daily_range_lower = np.percentile(daily_predictions, 2.5)
daily_range_upper = np.percentile(daily_predictions, 97.5)
print(f"Prediction Range: {daily_range_lower:.2f} - {daily_range_upper:.2f}")

# ==========================
# Monthly Prediction and Scenario
# ==========================
# Aggregate data for monthly modeling
monthly_data = data.groupby('month').agg({
    'brent_price': 'mean',
    'laden_perc': 'mean',
    'cum_count_vlcc': 'max',
    'td_rate': 'mean'  # Target
}).reset_index()

monthly_features = monthly_data.drop(columns=['td_rate', 'month'])
monthly_target = monthly_data['td_rate']

# Split into train and test sets
X_monthly_train, X_monthly_test, y_monthly_train, y_monthly_test = train_test_split(
    monthly_features, monthly_target, test_size=0.2, random_state=42)

# Train a regression model
from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_monthly_train, y_monthly_train)

# Predict and evaluate
monthly_predictions = rf_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly Prediction MAE: {mae_monthly:.2f}")


#==============


# Import required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load the datasets (replace with your actual file paths)
ais_data = pd.read_csv("ais_data.csv")
crude_prices = pd.read_csv("crude_prices.csv")
market_data = pd.read_csv("market_data.csv")
supply_demand = pd.read_csv("supply_demand.csv")

# Merge datasets on the 'date' column
data = pd.merge(ais_data, crude_prices, on="date")
data = pd.merge(data, market_data, on="date")
data = pd.merge(data, supply_demand, on="date")

# Convert 'date' to datetime
data['date'] = pd.to_datetime(data['date'])

# Create month-year column for aggregation purposes
data['month'] = data['date'].dt.to_period('M')

# ==========================
# Feature Engineering
# ==========================

# Create lag features
data['brent_price_lag1'] = data['brent_price'].shift(1)
data['laden_perc_7day_avg'] = data['laden_perc'].rolling(window=7).mean()

# Add cumulative count of VLCCs within a month
data['cum_count_vlcc'] = data.groupby('month')['Count_VLCC'].cumsum()

# Add scenario feature for situational analysis
data['Exp_TD3C_KBD_scenario'] = data['Exp_TD3C_KBD'] + 500

# Drop rows with NaN after feature creation
data = data.dropna()

# ==========================
# Preparing Data for Daily Predictions
# ==========================
# Define daily features and target
daily_features = data.drop(columns=['td_rate', 'date', 'month'])
daily_target = data['td_rate']

# Split data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(daily_features, daily_target, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Reshape input for LSTM (3D input: [samples, timesteps, features])
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)
X_test_lstm = X_test_tensor.unsqueeze(1)

# ==========================
# Define LSTM Model
# ==========================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last time step output
        return output

# Initialize model, loss function, and optimizer
input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = 1

model = LSTMModel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================
# Train the LSTM Model
# ==========================
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        val_output = model(X_val_lstm)
        val_loss = criterion(val_output, y_val_tensor)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

# ==========================
# Evaluate Daily Prediction
# ==========================
model.eval()
with torch.no_grad():
    predictions = model(X_test_lstm).numpy()
mae_daily = mean_absolute_error(y_test, predictions)
rmse_daily = mean_squared_error(y_test, predictions, squared=False)
print(f"Daily Prediction MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}")

# ==========================
# Preparing Data for Monthly Predictions
# ==========================
monthly_data = data.groupby('month').agg({
    'brent_price': 'mean',
    'laden_perc': 'mean',
    'cum_count_vlcc': 'max',  # Use total VLCC count
    'td_rate': 'mean'  # Target for monthly prediction
}).reset_index()

monthly_features = monthly_data.drop(columns=['td_rate', 'month'])
monthly_target = monthly_data['td_rate']

X_monthly_train, X_monthly_test, y_monthly_train, y_monthly_test = train_test_split(
    monthly_features, monthly_target, test_size=0.2, random_state=42)

# Train a simple regression model for monthly predictions
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_monthly_train, y_monthly_train)

# Predict and evaluate
monthly_predictions = rf_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly Prediction MAE: {mae_monthly:.2f}")

# ==========================
# Scenario Analysis
# ==========================
scenario_data = data.copy()
scenario_data['Exp_TD3C_KBD'] += 1500
scenario_features = scaler.transform(scenario_data.drop(columns=['td_rate', 'date', 'month']))
scenario_features_tensor = torch.tensor(scenario_features, dtype=torch.float32).unsqueeze(1)

# Predict scenario
model.eval()
with torch.no_grad():
    scenario_predictions = model(scenario_features_tensor).numpy()

# Compute range
scenario_lower = np.percentile(scenario_predictions, 2.5)
scenario_upper = np.percentile(scenario_predictions, 97.5)
print(f"Scenario Price Range: {scenario_lower:.2f} - {scenario_upper:.2f}")




#==============================================================


import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Load your data
ais_data = pd.read_csv("ais_data.csv")
crude_prices = pd.read_csv("crude_prices.csv")
market_data = pd.read_csv("market_data.csv")
supply_demand = pd.read_csv("supply_demand.csv")

# Merge datasets on date
data = pd.merge(ais_data, crude_prices, on="date")
data = pd.merge(data, market_data, on="date")
data = pd.merge(data, supply_demand, on="date")


# Example feature engineering
data['brent_price_lag1'] = data['brent_price'].shift(1)
data['laden_perc_7day_avg'] = data['laden_perc'].rolling(window=7).mean()
data['Exp_TD3C_KBD_scenario'] = data['Exp_TD3C_KBD'] + 500

# Drop rows with NaN after feature creation
data = data.dropna()

# Define target and features
features = data.drop(columns=['td_rate', 'date'])
target = data['td_rate']


# Split data
X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)


class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last time step output
        return output


# Initialize model, loss, and optimizer
input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = 1

model = LSTMModel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Reshape input for LSTM (3D input: [samples, timesteps, features])
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)

# Training loop
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    # Validation
    model.eval()
    val_output = model(X_val_lstm)
    val_loss = criterion(val_output, y_val_tensor)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")


# Reshape test data
X_test_lstm = X_test_tensor.unsqueeze(1)

# Predict and evaluate
model.eval()
predictions = model(X_test_lstm).detach().numpy()
mae = mean_absolute_error(y_test, predictions)
rmse = mean_squared_error(y_test, predictions, squared=False)
print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}")


# Aggregate to monthly
data['month'] = pd.to_datetime(data['date']).dt.to_period('M')
monthly_data = data.groupby('month').mean().reset_index()

# Prepare monthly features
monthly_features = monthly_data.drop(columns=['td_rate', 'month'])
monthly_target = monthly_data['td_rate']

X_monthly_train, X_monthly_test, y_monthly_train, y_monthly_test = train_test_split(
    monthly_features, monthly_target, test_size=0.2, random_state=42)

# Train a simple linear regression for monthly predictions
from sklearn.linear_model import LinearRegression
lr_model = LinearRegression()
lr_model.fit(X_monthly_train, y_monthly_train)

# Predict and evaluate
monthly_predictions = lr_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly MAE: {mae_monthly:.2f}")


# Compute 95% confidence interval for daily predictions
lower_bound = np.percentile(predictions, 2.5)
upper_bound = np.percentile(predictions, 97.5)
print(f"Predicted daily price range: {lower_bound:.2f} - {upper_bound:.2f}")


# Simulate scenario
scenario_data = data.copy()
scenario_data['Exp_TD3C_KBD'] += 1500  # Example scenario
scenario_features = scaler.transform(scenario_data.drop(columns=['td_rate', 'date']))

# Predict for scenario
scenario_features_tensor = torch.tensor(scenario_features, dtype=torch.float32).unsqueeze(1)
scenario_predictions = model(scenario_features_tensor).detach().numpy()

# Compute range for the scenario
scenario_lower = np.percentile(scenario_predictions, 2.5)
scenario_upper = np.percentile(scenario_predictions, 97.5)
print(f"Scenario price range: {scenario_lower:.2f} - {scenario_upper:.2f}")


