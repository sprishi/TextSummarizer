
import streamlit as st

# Add container-specific scaling
st.markdown("""
<style>
    .scaled-container {
        transform: scale(0.85);
        transform-origin: top left;
        width: 117.65%; /* 100% / 0.85 */
        margin-left: -8.5%; /* (1 - 0.85) / 2 */
        padding-bottom: 15%; /* Prevent content clipping */
    }
    
    /* Fix checkbox sizes */
    .scaled-container .stCheckbox label {
        font-size: 1.15em;
        transform: scale(1.15);
        transform-origin: left;
    }
    
    /* Fix button sizes */
    .scaled-container .stButton button {
        transform: scale(1.15);
        transform-origin: left;
    }
</style>
""", unsafe_allow_html=True)

# Your content inside scaled container
with st.markdown('<div class="scaled-container">', unsafe_allow_html=True):
    with st.container():
        # Example: Grid of checkboxes
        cols = st.columns(5)
        for i in range(20):
            with cols[i % 5]:
                st.checkbox(f"Option {i+1}", key=f"opt_{i}")
        
        # Example: Chart
        st.line_chart({"data": [1, 5, 2, 6, 2, 1]})
        
        # Example: Table
        st.dataframe({
            "Column 1": [1, 2, 3, 4, 5],
            "Column 2": ["A", "B", "C", "D", "E"]
        })

# Close the scaled container
st.markdown('</div>', unsafe_allow_html=True)
#>>>>>>>>


import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from pmdarima import auto_arima
from prophet import Prophet
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
warnings.filterwarnings('ignore')

# --------------------------------------------------
# 1. Data Preparation & Feature Selection
# --------------------------------------------------
def load_data(freq='W'):
    """Load preprocessed weekly/monthly data"""
    df = pd.read_csv(f'td3c_{freq}.csv', parse_dates=['Date'], index_col='Date')
    df = df.asfreq(freq)  # Set frequency
    df = df.sort_index()
    
    # Remove incomplete periods
    cutoff = df.index.max() - (pd.DateOffset(weeks=1) if freq == 'W' else pd.DateOffset(months=1))
    df = df[df.index <= cutoff]
    
    # Create directional target
    df[f'Direction'] = np.where(df[f'WS_Spot_next_{freq[0].lower()}'].diff() > 0, 1, 0)
    return df

weekly_data = load_data('W')
monthly_data = load_data('M')

# --------------------------------------------------
# 2. Train-Validation-Test Split (Time-Based)
# --------------------------------------------------
def temporal_split(df, val_size=0.2, test_size=0.2):
    dates = df.index.sort_values()
    test_cutoff = dates[-int(len(dates)*test_size)]
    val_cutoff = dates[-int(len(dates)*(test_size+val_size))]
    
    train = df.loc[:val_cutoff - timedelta(days=1)]
    val = df.loc[val_cutoff:test_cutoff - timedelta(days=1)]
    test = df.loc[test_cutoff:]
    return train, val, test

# Weekly split
train_w, val_w, test_w = temporal_split(weekly_data)
# Monthly split
train_m, val_m, test_m = temporal_split(monthly_data)

# --------------------------------------------------
# 3. Model Building & Evaluation Framework
# --------------------------------------------------
class TSModel:
    def __init__(self, freq='W'):
        self.freq = freq
        self.model = None
        
    def train(self, train_df, val_df=None):
        """Train model with validation for parameter tuning"""
        pass
    
    def predict(self, test_df):
        """Return predictions with confidence intervals"""
        pass
    
    def evaluate(self, actual, preds, ci):
        """Calculate metrics and plot results"""
        metrics = {
            'MAE': mean_absolute_error(actual, preds),
            'RMSE': np.sqrt(mean_squared_error(actual, preds)),
            'Direction_Acc': accuracy_score(actual['Direction'], (preds.diff() > 0).astype(int))
        }
        
        plt.figure(figsize=(12,6))
        plt.plot(actual.index, actual[f'WS_Spot_next_{self.freq[0].lower()}'], label='Actual')
        plt.plot(actual.index, preds, label='Predicted')
        plt.fill_between(actual.index, ci[:,0], ci[:,1], alpha=0.2)
        plt.title(f'{self.__class__.__name__} {self.freq} Forecast Performance')
        plt.legend()
        plt.show()
        
        return metrics

# --------------------------------------------------
# 4. Auto ARIMA Implementation
# --------------------------------------------------
class ARIMAModel(TSModel):
    def train(self, train_df, val_df=None):
        y_train = train_df[f'WS_Spot_next_{self.freq[0].lower()}']
        X_train = train_df.drop(columns=[f'WS_Spot_next_{self.freq[0].lower()}','Direction'])
        
        self.model = auto_arima(
            y_train, exogenous=X_train,
            seasonal=True, m=4 if self.freq == 'W' else 12,
            stepwise=True, trace=True,
            error_action='ignore'
        )
        return self
    
    def predict(self, test_df):
        X_test = test_df.drop(columns=[f'WS_Spot_next_{self.freq[0].lower()}','Direction'])
        preds, conf_int = self.model.predict(
            n_periods=len(test_df),
            exogenous=X_test,
            return_conf_int=True
        )
        return preds, conf_int

# --------------------------------------------------
# 5. Prophet Implementation
# --------------------------------------------------
class ProphetModel(TSModel):
    def train(self, train_df, val_df=None):
        self.model = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=(self.freq == 'W'),
            uncertainty_samples=100
        )
        
        # Add all regressors except targets
        for col in train_df.columns:
            if col not in [f'WS_Spot_next_{self.freq[0].lower()}','Direction']:
                self.model.add_regressor(col)
                
        train_data = train_df.reset_index().rename(
            columns={'Date':'ds', f'WS_Spot_next_{self.freq[0].lower()}':'y'}
        )
        self.model.fit(train_data)
        return self
    
    def predict(self, test_df):
        future = self.model.make_future_dataframe(
            periods=len(test_df),
            freq=self.freq,
            include_history=False
        )
        
        # Merge exogenous variables
        future = future.merge(
            test_df.reset_index(),
            left_on='ds', right_on='Date'
        )
        
        forecast = self.model.predict(future)
        return forecast['yhat'].values, forecast[['yhat_lower','yhat_upper']].values

# --------------------------------------------------
# 6. Model Execution & Validation
# --------------------------------------------------
def run_pipeline(freq='W'):
    # Initialize models
    models = [ARIMAModel(freq), ProphetModel(freq)]
    results = {}
    
    # Load data
    train, val, test = temporal_split(weekly_data) if freq == 'W' else temporal_split(monthly_data)
    
    for model in models:
        print(f'\nTraining {model.__class__.__name__} ({freq})...')
        # Train on train+val (for final model)
        full_train = pd.concat([train, val])
        model.train(full_train)
        
        # Predict on test
        preds, ci = model.predict(test)
        actual = test[[f'WS_Spot_next_{freq[0].lower()}','Direction']]
        
        # Evaluate
        metrics = model.evaluate(actual, preds, ci)
        results[model.__class__.__name__] = {
            'metrics': metrics,
            'predictions': preds,
            'ci': ci
        }
    
    return results

# Run for both frequencies
weekly_results = run_pipeline('W')
monthly_results = run_pipeline('M')

# --------------------------------------------------
# 7. Full Retraining & Final Predictions
# --------------------------------------------------
def retrain_and_predict(df, model_class, freq='W'):
    model = model_class(freq)
    model.train(df)
    
    # Create future dataframe
    last_date = df.index.max()
    future_date = last_date + (7 if freq == 'W' else 30)
    
    # For ARIMA - need exogenous variables
    if model_class == ARIMAModel:
        X_future = df.drop(columns=[f'WS_Spot_next_{freq[0].lower()}','Direction']).iloc[[-1]]
        pred, ci = model.model.predict(n_periods=1, exogenous=X_future, return_conf_int=True)
    else:
        future = pd.DataFrame({'ds': [future_date]})
        future = future.merge(df.reset_index(), how='cross').iloc[:1]  # Propagate features
        forecast = model.model.predict(future)
        pred, ci = forecast['yhat'].values[0], forecast[['yhat_lower','yhat_upper']].values[0]
    
    return {
        'date': future_date,
        'prediction': pred,
        'lower': ci[0],
        'upper': ci[1],
        'direction': 'Up' if pred > df.iloc[-1][f'WS_Spot_next_{freq[0].lower()}'] else 'Down'
    }

# Example usage for next week prediction
final_weekly_pred = retrain_and_predict(weekly_data, ARIMAModel, 'W')
print("\nFinal Weekly Prediction:")
print(final_weekly_pred)

# --------------------------------------------------
# 8. Scenario Analysis Engine
# --------------------------------------------------
def run_scenario(base_df, model_class, scenario_params, freq='W'):
    """Simulate different export scenarios
    scenario_params: dict of {variable: [(change_value, change_type)]}
    Example: {'Exp_TD3C_KBD': [(500, 'absolute'), (1500, 'absolute')]}
    """
    results = []
    model = model_class(freq)
    model.train(base_df)
    
    for var, changes in scenario_params.items():
        for change in changes:
            # Clone last period data
            future_data = base_df.iloc[[-1]].copy()
            
            # Apply change
            if change[1] == 'absolute':
                future_data[var] += change[0]
            elif change[1] == 'percentage':
                future_data[var] *= (1 + change[0]/100)
            
            # Predict
            if model_class == ARIMAModel:
                pred, ci = model.model.predict(
                    n_periods=1,
                    exogenous=future_data.drop(columns=[f'WS_Spot_next_{freq[0].lower()}','Direction']),
                    return_conf_int=True
                )
            else:
                future_ds = pd.DataFrame({'ds': [base_df.index[-1] + (7 if freq=='W' else 30)]})
                future_df = future_ds.merge(future_data.reset_index(), how='cross')
                forecast = model.model.predict(future_df)
                pred, ci = forecast['yhat'].values[0], forecast[['yhat_lower','yhat_upper']].values[0]
            
            results.append({
                'variable': var,
                'change': change,
                'prediction': pred,
                'ci_lower': ci[0],
                'ci_upper': ci[1]
            })
    
    return pd.DataFrame(results)

# Example scenario analysis
scenario_results = run_scenario(
    weekly_data,
    ARIMAModel,
    scenario_params={
        'Exp_TD3C_KBD': [(500, 'absolute'), (1500, 'absolute')],
        'laden_perc': [(10, 'percentage')]
    },
    freq='W'
)

print("\nScenario Analysis Results:")
print(scenario_results)

#========


# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pmdarima import auto_arima
from prophet import Prophet
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score
from statsmodels.stats.outliers_influence import variance_inflation_factor

# ----------------------------
# 1. Data Loading & Preparation
# ----------------------------
# Load preprocessed data
weekly = pd.read_csv('weekly_data.csv', parse_dates=['Date'], index_col='Date')
monthly = pd.read_csv('monthly_data.csv', parse_dates=['Date'], index_col='Date')

# Final feature columns (excluding targets)
features_weekly = weekly.columns.tolist()[:-1]  # Assuming last column is target
features_monthly = monthly.columns.tolist()[:-1]

# ----------------------------
# 2. Train-Test Split (Time-Based)
# ----------------------------
def time_split(df, test_size=0.2):
    split_idx = int(len(df)*(1-test_size))
    return df.iloc[:split_idx], df.iloc[split_idx:]

# Weekly split
train_week, test_week = time_split(weekly)
X_train_week, y_train_week = train_week[features_weekly], train_week.iloc[:,-1]
X_test_week, y_test_week = test_week[features_weekly], test_week.iloc[:,-1]

# Monthly split
train_month, test_month = time_split(monthly)
X_train_month, y_train_month = train_month[features_monthly], train_month.iloc[:,-1]
X_test_month, y_test_month = test_month[features_monthly], test_month.iloc[:,-1]

# ----------------------------
# 3. Model Building & Evaluation
# ----------------------------
def run_arima(X_train, y_train, X_test, freq='W'):
    model = auto_arima(y_train, exogenous=X_train,
                      seasonal=True, m=4 if freq=='W' else 12,
                      stepwise=True, suppress_warnings=True)
    preds, conf_int = model.predict(n_periods=len(X_test), 
                                  exogenous=X_test, 
                                  return_conf_int=True)
    return model, preds, conf_int

def run_prophet(train, test, freq='W'):
    df = train.reset_index().rename(columns={'Date':'ds', train.columns[-1]:'y'})
    m = Prophet(weekly_seasonality=(freq=='W'), yearly_seasonality=True)
    
    for col in features_weekly if freq=='W' else features_monthly:
        m.add_regressor(col)
        
    m.fit(df)
    
    future = m.make_future_dataframe(periods=len(test), freq=freq)
    future = future.merge(test.reset_index(), on='Date', how='right')
    forecast = m.predict(future)
    return m, forecast.yhat.values, forecast[['yhat_lower', 'yhat_upper']].values

# Weekly Models
arima_week_model, arima_week_preds, arima_week_ci = run_arima(X_train_week, y_train_week, X_test_week)
prophet_week_model, prophet_week_preds, prophet_week_ci = run_prophet(train_week, test_week)

# Monthly Models
arima_month_model, arima_month_preds, arima_month_ci = run_arima(X_train_month, y_train_month, X_test_month)
prophet_month_model, prophet_month_preds, prophet_month_ci = run_prophet(train_month, test_month)

# ----------------------------
# 4. Model Evaluation Metrics
# ----------------------------
def evaluate_results(actual, preds, ci, model_name):
    results = pd.DataFrame({
        'Actual': actual.values,
        'Predicted': preds,
        'Lower': ci[:,0],
        'Upper': ci[:,1]
    }, index=actual.index)
    
    # Direction Accuracy
    results['Actual_Direction'] = (results['Actual'].diff() > 0).astype(int)
    results['Predicted_Direction'] = (results['Predicted'].diff() > 0).astype(int)
    
    print(f"\n{model_name} Evaluation:")
    print(f"MAE: {mean_absolute_error(results.Actual, results.Predicted):.2f}")
    print(f"RMSE: {np.sqrt(mean_squared_error(results.Actual, results.Predicted)):.2f}")
    print(f"Direction Accuracy: {accuracy_score(results.Actual_Direction[1:], results.Predicted_Direction[1:]):.2%}")
    
    return results

# Evaluate all models
week_arima_results = evaluate_results(y_test_week, arima_week_preds, arima_week_ci, "Weekly ARIMA")
week_prophet_results = evaluate_results(y_test_week, prophet_week_preds, prophet_week_ci, "Weekly Prophet")
month_arima_results = evaluate_results(y_test_month, arima_month_preds, arima_month_ci, "Monthly ARIMA")
month_prophet_results = evaluate_results(y_test_month, prophet_month_preds, prophet_month_ci, "Monthly Prophet")

# ----------------------------
# 5. Visualization
# ----------------------------
def plot_predictions(results, title):
    plt.figure(figsize=(12,6))
    plt.plot(results.index, results.Actual, label='Actual', marker='o')
    plt.plot(results.index, results.Predicted, label='Predicted', linestyle='--')
    plt.fill_between(results.index, results.Lower, results.Upper, alpha=0.2)
    plt.title(f"{title}\nMAE: {mean_absolute_error(results.Actual, results.Predicted):.2f}")
    plt.legend()
    plt.grid(True)
    plt.show()

plot_predictions(week_arima_results, "Weekly ARIMA Predictions")
plot_predictions(week_prophet_results, "Weekly Prophet Predictions")
plot_predictions(month_arima_results, "Monthly ARIMA Predictions")
plot_predictions(month_prophet_results, "Monthly Prophet Predictions")

# ----------------------------
# 6. Full Retraining & Final Predictions
# ----------------------------
def final_retrain(df, model_type='arima', freq='W'):
    X = df[features_weekly if freq=='W' else features_monthly]
    y = df.iloc[:,-1]
    
    if model_type == 'arima':
        model = auto_arima(y, exogenous=X, seasonal=True, m=4 if freq=='W' else 12)
        next_date = df.index[-1] + pd.DateOffset(weeks=1 if freq=='W' else months=1)
        pred, ci = model.predict(n_periods=1, exogenous=X.iloc[[-1]], return_conf_int=True)
    else:
        prophet_df = df.reset_index().rename(columns={'Date':'ds', df.columns[-1]:'y'})
        m = Prophet(weekly_seasonality=(freq=='W'), yearly_seasonality=True)
        for col in X.columns:
            m.add_regressor(col)
        m.fit(prophet_df)
        future = m.make_future_dataframe(periods=1, freq=freq)
        future = future.merge(X.iloc[[-1]].reset_index(), on='Date', how='left')
        forecast = m.predict(future)
        pred = forecast.yhat.iloc[-1]
        ci = forecast[['yhat_lower', 'yhat_upper']].iloc[-1].values
        next_date = future.ds.iloc[-1]
    
    return pd.DataFrame({
        'Date': [next_date],
        'Predicted': [pred],
        'Lower': [ci[0]],
        'Upper': [ci[1]],
        'Direction': ['Up' if pred > df.iloc[-1,-1] else 'Down']
    })

# Generate final predictions
final_week_arima = final_retrain(weekly, 'arima', 'W')
final_week_prophet = final_retrain(weekly, 'prophet', 'W')
final_month_arima = final_retrain(monthly, 'arima', 'M')
final_month_prophet = final_retrain(monthly, 'prophet', 'M')

# ----------------------------
# 7. Scenario Analysis
# ----------------------------
def scenario_analysis(base_data, kbd_increase, model_type='arima', freq='W'):
    # Create scenario data
    scenario_data = base_data.copy()
    last_date = scenario_data.index[-1]
    
    # Adjust exports
    scenario_data['Exp_TD3C_KBD'] += kbd_increase
    
    # Get prediction
    if model_type == 'arima':
        model = auto_arima(base_data.iloc[:,-1], 
                          exogenous=base_data[features_weekly if freq=='W' else features_monthly],
                          seasonal=True)
        pred, ci = model.predict(n_periods=1, 
                                exogenous=scenario_data.iloc[[-1],:-1], 
                                return_conf_int=True)
    else:
        prophet_df = base_data.reset_index().rename(columns={'Date':'ds', base_data.columns[-1]:'y'})
        m = Prophet()
        for col in features_weekly if freq=='W' else features_monthly:
            m.add_regressor(col)
        m.fit(prophet_df)
        future = m.make_future_dataframe(periods=1, freq=freq)
        future = future.merge(scenario_data.iloc[[-1],:-1].reset_index(), 
                            on='Date', how='left')
        forecast = m.predict(future)
        pred = forecast.yhat.iloc[-1]
        ci = forecast[['yhat_lower', 'yhat_upper']].iloc[-1].values
    
    return pred, ci

# Example: 1500 KBD increase scenario
kbd_scenario = 1500
base_case = weekly.iloc[-1:].copy()  # Use last available week

# Weekly ARIMA scenario
scenario_pred, scenario_ci = scenario_analysis(base_case, kbd_scenario)
print(f"\nScenario Analysis (1500 KBD Increase):")
print(f"Predicted Price: {scenario_pred[0]:.2f}")
print(f"Confidence Interval: {scenario_ci[0][0]:.2f} - {scenario_ci[0][1]:.2f}")



#=======================

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, accuracy_score
from torch.utils.data import Dataset, DataLoader

# --------------------
# 1. Data Preparation
# --------------------
# Load your dataframe (replace with actual data loading)
df = pd.read_csv('td3c_data.csv', parse_dates=['date'])

# Feature Engineering
def preprocess_data(df):
    # Shift target for multi-horizon forecasting
    df['WS_Spot_next_day'] = df['WS_Spot'].shift(-1)
    df['WS_Spot_next_week'] = df['WS_Spot'].shift(-7)
    df['WS_Spot_next_month'] = df['WS_Spot'].shift(-30)
    
    # Handle monthly features (example: Exp_TD3C_KBD)
    df['Exp_TD3C_KBD'] = df['Exp_TD3C_KBD'].ffill()  # Forward fill monthly data
    
    # Lag features
    for lag in [1, 7, 30]:
        df[f'WS_Spot_lag_{lag}'] = df['WS_Spot'].shift(lag)
        df[f'Brent_lag_{lag}'] = df['brent_price'].shift(lag)
    
    # Rolling features
    df['Count_VLCC_7d_avg'] = df['Count_VLCC'].rolling(7).mean()
    
    # Event dummies (example: COVID period)
    df['event_covid'] = ((df['date'] >= '2020-03-01') & (df['date'] <= '2021-06-01')).astype(int)
    
    # Drop rows with NaNs from shifting
    df = df.dropna()
    return df

df = preprocess_data(df)

# --------------------
# 2. Train/Val/Test Split
# --------------------
def time_based_split(df, train_end='2023-12-31', val_end='2024-06-30'):
    train = df[df['date'] <= train_end]
    val = df[(df['date'] > train_end) & (df['date'] <= val_end)]
    test = df[df['date'] > val_end]
    return train, val, test

train_df, val_df, test_df = time_based_split(df)

# --------------------
# 3. Create Sequences
# --------------------
class TimeSeriesDataset(Dataset):
    def __init__(self, df, window_size=30):
        self.window_size = window_size
        features = df.drop(columns=['date', 'WS_Spot_next_day', 'WS_Spot_next_week', 'WS_Spot_next_month'])
        targets = df[['WS_Spot_next_day', 'WS_Spot_next_week', 'WS_Spot_next_month']]
        
        self.X = []
        self.y = []
        for i in range(len(df) - window_size):
            self.X.append(features.iloc[i:i+window_size].values)
            self.y.append(targets.iloc[i+window_size].values)
        
        self.X = torch.tensor(np.array(self.X), dtype=torch.float32)
        self.y = torch.tensor(np.array(self.y), dtype=torch.float32)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

window_size = 30
train_dataset = TimeSeriesDataset(train_df, window_size)
val_dataset = TimeSeriesDataset(val_df, window_size)
test_dataset = TimeSeriesDataset(test_df, window_size)

# --------------------
# 4. LSTM Model (Multi-Output)
# --------------------
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, output_size=3):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(0.3)
        self.linear = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:, -1, :]  # Take last timestep
        x = self.dropout(x)
        x = self.linear(x)
        return x

# Initialize model
input_size = len(train_df.columns) - 4  # Exclude date and targets
model = LSTMModel(input_size=input_size)
criterion = nn.L1Loss()  # MAE loss
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# --------------------
# 5. Training Loop
# --------------------
def train_model(model, train_loader, val_loader, epochs=100):
    best_loss = float('inf')
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_val, y_val in val_loader:
                output = model(X_val)
                val_loss += criterion(output, y_val).item()
        
        # Early stopping
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')
        
        print(f'Epoch {epoch+1}: Train MAE {train_loss/len(train_loader):.2f}, Val MAE {val_loss/len(val_loader):.2f}')

# Create DataLoaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Start training
train_model(model, train_loader, val_loader, epochs=100)

# --------------------
# 6. Evaluation & Metrics
# --------------------
def evaluate_model(model, test_loader):
    model.eval()
    predictions = []
    actuals = []
    with torch.no_grad():
        for X_test, y_test in test_loader:
            output = model(X_test)
            predictions.append(output.numpy())
            actuals.append(y_test.numpy())
    
    predictions = np.concatenate(predictions)
    actuals = np.concatenate(actuals)
    
    # MAE
    mae_day = mean_absolute_error(actuals[:, 0], predictions[:, 0])
    mae_week = mean_absolute_error(actuals[:, 1], predictions[:, 1])
    mae_month = mean_absolute_error(actuals[:, 2], predictions[:, 2])
    
    # Directional Accuracy
    direction_day = accuracy_score((actuals[:, 0] > 0), (predictions[:, 0] > 0))
    direction_week = accuracy_score((actuals[:, 1] > 0), (predictions[:, 1] > 0))
    direction_month = accuracy_score((actuals[:, 2] > 0), (predictions[:, 2] > 0))
    
    return {
        'mae': (mae_day, mae_week, mae_month),
        'direction_accuracy': (direction_day, direction_week, direction_month)
    }

test_loader = DataLoader(test_dataset, batch_size=batch_size)
metrics = evaluate_model(model, test_loader)
print(f"Test MAE: Day {metrics['mae'][0]:.2f}, Week {metrics['mae'][1]:.2f}, Month {metrics['mae'][2]:.2f}")
print(f"Direction Accuracy: Day {metrics['direction_accuracy'][0]:.2%}, Week {metrics['direction_accuracy'][1]:.2%}, Month {metrics['direction_accuracy'][2]:.2%}")

# --------------------
# 7. Prediction Intervals (MC Dropout)
# --------------------
def predict_with_uncertainty(model, X, n_iter=100):
    model.train()  # Keep dropout active
    predictions = []
    with torch.no_grad():
        for _ in range(n_iter):
            predictions.append(model(X).numpy())
    return np.array(predictions)

# Example usage on test data
X_test_sample = test_dataset[0][0].unsqueeze(0)
mc_predictions = predict_with_uncertainty(model, X_test_sample)
print(f"95% Prediction Interval (Day): {np.percentile(mc_predictions[:, 0], [2.5, 97.5])}")

# --------------------
# 8. Scenario Analysis
# --------------------
def scenario_analysis(base_data, export_increase=500):
    # Clone and modify export data
    scenario_data = base_data.clone()
    scenario_data[:, :, df.columns.get_loc('Exp_TD3C_KBD')] += export_increase
    return model(scenario_data)

# Example: Predict with +1500 KBD exports
base_input = test_dataset[-1][0].unsqueeze(0)  # Last available data point
scenario_pred = scenario_analysis(base_input, 1500)
print(f"Scenario Prediction: {scenario_pred.detach().numpy()}")

# --------------------
# 9. Final Predictions (Jan 14, 2025+)
# --------------------
# Retrain on full data
full_dataset = TimeSeriesDataset(df, window_size)
full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)
train_model(model, full_loader, val_loader, epochs=50)  # Final tuning

# Get latest window
latest_window = torch.tensor(df.iloc[-window_size:].drop(columns=['date', 'WS_Spot_next_day', 'WS_Spot_next_week', 'WS_Spot_next_month']).values,
                            dtype=torch.float32).unsqueeze(0)

# Predict future
with torch.no_grad():
    predictions = model(latest_window)
    print(f"Next day: {predictions[0,0]:.2f}, Week: {predictions[0,1]:.2f}, Month: {predictions[0,2]:.2f}")


# =============================================================================================


import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error
from torch.utils.data import Dataset, DataLoader, TensorDataset

# =============================================
# Step 1: Data Preparation & Feature Engineering
# =============================================

# Mock dataframe structure - replace with your actual data loading
# Assuming df has columns: ['date', 'WS_Spot', 'laden_perc', 'spread_perc_global', ...]
df = pd.read_csv('your_data.csv', parse_dates=['date'])

# 1.1 Handle Monthly Export Data (Exp_TD3C_KBD)
df['Exp_TD3C_KBD'] = df['Exp_TD3C_KBD'].ffill()  # Forward fill monthly values to daily

# 1.2 Smooth Count_VLCC with 7-day rolling average
df['Count_VLCC'] = df['Count_VLCC'].rolling(7, min_periods=1).mean()

# 1.3 Create Lag Features (7-day lags)
lag_days = 7
features_to_lag = ['brent_price', 'BDTI', 'MFO_Fuj', 'laden_perc']
for feat in features_to_lag:
    for lag in range(1, lag_days+1):
        df[f'{feat}_lag{lag}'] = df[feat].shift(lag)

# 1.4 Event Dummies (Add your actual event dates)
events = {
    'covid': ('2020-03-01', '2022-12-31'), 
    'russia_war': ('2022-02-24', '2023-12-31')
}
df['event_dummy'] = 0
for event, (start, end) in events.items():
    df.loc[df['date'].between(start, end), 'event_dummy'] = 1

# 1.5 Final Features & Target
features = ['brent_price', 'BDTI', 'MFO_Fuj', 'laden_perc', 'spread_perc_global',
            'Count_VLCC', 'Exp_TD3C_KBD', 'USD_SAR', 'USD_CNY', 'event_dummy'] + \
           [f'{feat}_lag{i}' for feat in features_to_lag for i in range(1, lag_days+1)]
target = 'WS_Spot'

# 1.6 Split Data (Time-based)
train_size = int(0.7 * len(df))
val_size = int(0.15 * len(df))
test_size = len(df) - train_size - val_size

train_df = df.iloc[:train_size]
val_df = df.iloc[train_size:train_size+val_size]
test_df = df.iloc[train_size+val_size:]

# 1.7 Normalization
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_df[features])
val_scaled = scaler.transform(val_df[features])
test_scaled = scaler.transform(test_df[features])

# =============================================
# Step 2: PyTorch Dataset & DataLoader
# =============================================

class TimeSeriesDataset(Dataset):
    def __init__(self, X, y, seq_length=60):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
        self.seq_length = seq_length

    def __len__(self):
        return len(self.X) - self.seq_length

    def __getitem__(self, idx):
        return (self.X[idx:idx+self.seq_length], 
                self.y[idx+self.seq_length])

# Create datasets
seq_length = 60  # 2 months lookback

train_dataset = TimeSeriesDataset(
    train_scaled, train_df[target].values, seq_length
)
val_dataset = TimeSeriesDataset(
    val_scaled, val_df[target].values, seq_length
)
test_dataset = TimeSeriesDataset(
    test_scaled, test_df[target].values, seq_length
)

# DataLoaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# =============================================
# Step 3: LSTM Model Architecture (Multi-Task)
# =============================================

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers, 
            batch_first=True, dropout=0.2
        )
        self.regressor = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_out = lstm_out[:, -1, :]
        return self.regressor(last_out), self.classifier(last_out)

model = LSTMModel(input_size=len(features))
criterion_reg = nn.MSELoss()
criterion_cls = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# =============================================
# Step 4: Training with Early Stopping
# =============================================

best_val_loss = float('inf')
patience, trials = 5, 0

for epoch in range(100):
    # Training
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        pred_reg, pred_cls = model(X_batch)
        loss_reg = criterion_reg(pred_reg.squeeze(), y_batch)
        loss_cls = criterion_cls(pred_cls.squeeze(), (y_batch > X_batch[:, -1, 0]).float())
        loss = 0.7*loss_reg + 0.3*loss_cls
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    # Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for X_val, y_val in val_loader:
            pred_reg, pred_cls = model(X_val)
            loss_reg = criterion_reg(pred_reg.squeeze(), y_val)
            loss_cls = criterion_cls(pred_cls.squeeze(), (y_val > X_val[:, -1, 0]).float())
            val_loss += (0.7*loss_reg + 0.3*loss_cls).item()
    
    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        trials = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        trials += 1
        if trials >= patience:
            print(f'Early stopping at epoch {epoch}')
            break

    print(f'Epoch {epoch}: Train Loss {train_loss/len(train_loader):.4f}, Val Loss {val_loss/len(val_loader):.4f}')

# =============================================
# Step 5: Final Training & Prediction
# =============================================

# 5.1 Retrain on full data
full_dataset = TimeSeriesDataset(
    scaler.transform(df[features]), df[target].values, seq_length
)
full_loader = DataLoader(full_dataset, batch_size=batch_size)

model.load_state_dict(torch.load('best_model.pth'))
optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Smaller LR for fine-tuning

# Train for few more epochs
for epoch in range(10):
    # Training loop similar to above...

# 5.2 Prediction Function
def predict_next_period(model, last_sequence, steps=30):
    model.eval()
    predictions = []
    current_seq = last_sequence.clone()
    
    with torch.no_grad():
        for _ in range(steps):
            pred_reg, _ = model(current_seq.unsqueeze(0))
            predictions.append(pred_reg.item())
            # Update sequence
            new_features = ...  # Update features (requires real-time data)
            current_seq = torch.cat([current_seq[1:], new_features])
    
    return predictions

# Get last sequence from full data
last_seq = torch.tensor(scaler.transform(df[features].iloc[-seq_length:].values), 
                       dtype=torch.float32)

# Predictions
next_day = predict_next_period(model, last_seq, steps=1)
next_week = predict_next_period(model, last_seq, steps=7)
next_month = predict_next_period(model, last_seq, steps=30)

# =============================================
# Step 6: Scenario Analysis (Example)
# =============================================

def run_scenario(df, export_increase=1500):
    # Clone data and modify exports
    scenario_df = df.copy()
    scenario_df['Exp_TD3C_KBD'] += export_increase
    # Repeat steps 1.6-1.7 for scenario data
    # Return predictions
    return predict_next_period(model, scenario_seq)





#----------------------------



import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor

# ==========================
# Load and Preprocess Data
# ==========================
# Load dataset
data = pd.read_csv('your_dataset.csv', parse_dates=['date'])
data = data.sort_values('date')

# Feature Engineering
data['td_rate_lag1'] = data['td_rate'].shift(1)
data['td_rate_lag7'] = data['td_rate'].shift(7)
data['td_rate_lag30'] = data['td_rate'].shift(30)
data['rolling_mean'] = data['td_rate'].rolling(window=20).mean()
data['rolling_std'] = data['td_rate'].rolling(window=20).std()
data['price_outlier'] = ((data['td_rate'] - data['rolling_mean']) > 2.25 * data['rolling_std']).astype(int)
data['price_outlier'] = data['price_outlier'].shift(-1).fillna(0)
data = data.dropna()

# Features and Target
features = ['td_rate_lag1', 'td_rate_lag7', 'td_rate_lag30', 'price_outlier', 'rolling_mean', 'rolling_std']
daily_target = data['td_rate']
daily_features = data[features]

# Split Data into Train, Validation, and Test Sets
X_train, X_temp, y_train, y_temp = train_test_split(daily_features, daily_target, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize Data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Reshape for LSTM
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)
X_test_lstm = X_test_tensor.unsqueeze(1)

# ==========================
# Define LSTM Model
# ==========================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])
        return output

input_dim = len(features)
hidden_dim = 64
output_dim = 1
model = LSTMModel(input_dim, hidden_dim, output_dim)

# Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================
# Train the Model
# ==========================
epochs = 50
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()

    # Validation loss
    model.eval()
    with torch.no_grad():
        val_output = model(X_val_lstm)
        val_loss = criterion(val_output, y_val_tensor)

    if epoch % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

# ==========================
# Evaluate the Model
# ==========================
model.eval()
with torch.no_grad():
    test_output = model(X_test_lstm).numpy()

# Calculate Metrics
mae = mean_absolute_error(y_test, test_output)
rmse = mean_squared_error(y_test, test_output, squared=False)
print(f"Test MAE: {mae:.2f}, Test RMSE: {rmse:.2f}")

# Calculate Directional Accuracy
actual_directions = np.sign(np.diff(y_test.values))
predicted_directions = np.sign(np.diff(test_output.flatten()))
directional_accuracy = (actual_directions == predicted_directions).mean() * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

# ==========================
# Create Evaluation Table
# ==========================
test_dates = data['date'].iloc[-len(test_output):]  # Match test set length
eval_table = pd.DataFrame({
    'Date': test_dates,
    'Actual Value': y_test.values,
    'Predicted Value': test_output.flatten(),
    'Predicted Lower': np.percentile(test_output, 2.5),
    'Predicted Upper': np.percentile(test_output, 97.5),
    'Predicted Direction': np.concatenate(([0], predicted_directions))  # Add a dummy value for alignment
})
print(eval_table)

# ==========================
# Monthly Predictions
# ==========================
data['month'] = data['date'].dt.to_period('M')
monthly_data = data.groupby('month').agg({
    'td_rate': 'mean',
    'td_rate_lag1': 'mean',
    'td_rate_lag7': 'mean',
    'td_rate_lag30': 'mean',
    'price_outlier': 'max'
}).reset_index()

monthly_features = ['td_rate_lag1', 'td_rate_lag7', 'td_rate_lag30', 'price_outlier']
X_monthly = scaler.fit_transform(monthly_data[monthly_features])
y_monthly = monthly_data['td_rate']

# Train-Test Split for Monthly Data
train_monthly_size = int(len(monthly_data) * 0.8)
X_monthly_train = X_monthly[:train_monthly_size]
y_monthly_train = y_monthly[:train_monthly_size]
X_monthly_test = X_monthly[train_monthly_size:]
y_monthly_test = y_monthly[train_monthly_size:]

# Train Random Forest for Monthly Predictions
monthly_model = RandomForestRegressor(n_estimators=100, random_state=42)
monthly_model.fit(X_monthly_train, y_monthly_train)

# Evaluate Monthly Predictions
monthly_predictions = monthly_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly MAE: {mae_monthly:.2f}")

# Monthly Evaluation Table
monthly_eval_table = pd.DataFrame({
    'Month': monthly_data['month'].iloc[train_monthly_size:].reset_index(drop=True),
    'Actual Value': y_monthly_test,
    'Predicted Value': monthly_predictions
})
print(monthly_eval_table)



##

# ==========================
# Retrain Model on Full Data
# ==========================
# Combine all data for training
X_full = scaler.fit_transform(daily_features)
y_full = daily_target.values

# Convert to tensors
X_full_tensor = torch.tensor(X_full, dtype=torch.float32).unsqueeze(1)  # Add time dimension
y_full_tensor = torch.tensor(y_full, dtype=torch.float32).view(-1, 1)

# Retrain LSTM Model
model = LSTMModel(input_dim=X_full.shape[1], hidden_dim=64, output_dim=1)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Early Stopping Parameters
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(500):  # Use fewer epochs for full data retraining
    model.train()
    optimizer.zero_grad()
    output = model(X_full_tensor)
    loss = criterion(output, y_full_tensor)
    loss.backward()
    optimizer.step()

    # Print training progress
    if epoch % 50 == 0:
        print(f"Epoch {epoch+1}/{500}, Loss: {loss.item():.4f}")

# ==========================
# Predict Next Data Point
# ==========================
# Use the last available data point as input for prediction
next_input = torch.tensor(X_full[-1], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and time dims

# Enable dropout during prediction for uncertainty estimation
model.train()

# Monte Carlo Dropout for Range Prediction
num_simulations = 100  # Number of forward passes
predictions = []

with torch.no_grad():
    for _ in range(num_simulations):
        predictions.append(model(next_input).item())

# Calculate Prediction Range
next_prediction = np.mean(predictions)
lower_bound = np.percentile(predictions, 2.5)
upper_bound = np.percentile(predictions, 97.5)

print(f"Next Data Point Prediction: {next_prediction:.2f}")
print(f"Prediction Range: {lower_bound:.2f} - {upper_bound:.2f}")






#==========

# Import required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load data (replace with actual file paths)
ais_data = pd.read_csv("ais_data.csv")
crude_prices = pd.read_csv("crude_prices.csv")
market_data = pd.read_csv("market_data.csv")
supply_demand = pd.read_csv("supply_demand.csv")

# Merge datasets on the 'date' column
data = pd.merge(ais_data, crude_prices, on="date")
data = pd.merge(data, market_data, on="date")
data = pd.merge(data, supply_demand, on="date")

# Convert 'date' to datetime
data['date'] = pd.to_datetime(data['date'])

# Create month-year column for aggregation purposes
data['month'] = data['date'].dt.to_period('M')

# ==========================
# Feature Engineering
# ==========================

# Lag features
data['brent_price_lag1'] = data['brent_price'].shift(1)
data['td_rate_lag1'] = data['td_rate'].shift(1)  # Include today's price to predict tomorrow
data['laden_perc_7day_avg'] = data['laden_perc'].rolling(window=7).mean()

# Cumulative and scenario features
data['cum_count_vlcc'] = data.groupby('month')['Count_VLCC'].cumsum()
data['Exp_TD3C_KBD_scenario'] = data['Exp_TD3C_KBD'] + 500  # Example scenario

# Dummy variable for global events
std_dev_threshold = 2.25
window_size = 20
rolling_mean = data['td_rate'].rolling(window=window_size).mean()
rolling_std = data['td_rate'].rolling(window=window_size).std()
data['price_outlier'] = ((data['td_rate'] - rolling_mean) > std_dev_threshold * rolling_std).astype(int)

# Shift dummy variable to account for impact on the next day
data['price_outlier'] = data['price_outlier'].shift(-1).fillna(0)

# Add an improved dummy variable (1: up, -1: down, 0: stable)
data['price_movement'] = 0
data.loc[data['td_rate'].diff() > 0, 'price_movement'] = 1
data.loc[data['td_rate'].diff() < 0, 'price_movement'] = -1

# Drop rows with NaN after feature creation
data = data.dropna()

# ==========================
# Train-Validation-Test Split
# ==========================
# Split into train, validation, and test sets
train_data = data[data['date'] < '2024-01-01']
val_data = data[(data['date'] >= '2024-01-01') & (data['date'] < '2024-07-01')]
test_data = data[data['date'] >= '2024-07-01']

X_train = train_data.drop(columns=['td_rate', 'date', 'month'])
y_train = train_data['td_rate']
X_val = val_data.drop(columns=['td_rate', 'date', 'month'])
y_val = val_data['td_rate']
X_test = test_data.drop(columns=['td_rate', 'date', 'month'])
y_test = test_data['td_rate']

# Normalize data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert to tensors for PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Reshape input for LSTM (3D input: [samples, timesteps, features])
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)
X_test_lstm = X_test_tensor.unsqueeze(1)

# ==========================
# Define LSTM Model
# ==========================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last time step output
        return output

# Initialize model, loss, and optimizer
input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = 1

model = LSTMModel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================
# Train the LSTM Model
# ==========================
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        val_output = model(X_val_lstm)
        val_loss = criterion(val_output, y_val_tensor)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

# ==========================
# Evaluate Daily Prediction
# ==========================
model.eval()
with torch.no_grad():
    daily_predictions = model(X_test_lstm).numpy()

# Calculate MAE
mae_daily = mean_absolute_error(y_test, daily_predictions)
print(f"Daily Prediction MAE: {mae_daily:.2f}")

# Calculate Directional Accuracy
y_test_direction = np.sign(y_test.diff().fillna(0))
daily_predictions_direction = np.sign(pd.Series(daily_predictions.flatten()).diff().fillna(0))
directional_accuracy = (y_test_direction == daily_predictions_direction).mean() * 100
print(f"Directional Accuracy: {directional_accuracy:.2f}%")

# Calculate Prediction Range
daily_range_lower = np.percentile(daily_predictions, 2.5)
daily_range_upper = np.percentile(daily_predictions, 97.5)
print(f"Prediction Range: {daily_range_lower:.2f} - {daily_range_upper:.2f}")

# ==========================
# Monthly Prediction and Scenario
# ==========================
# Aggregate data for monthly modeling
monthly_data = data.groupby('month').agg({
    'brent_price': 'mean',
    'laden_perc': 'mean',
    'cum_count_vlcc': 'max',
    'td_rate': 'mean'  # Target
}).reset_index()

monthly_features = monthly_data.drop(columns=['td_rate', 'month'])
monthly_target = monthly_data['td_rate']

# Split into train and test sets
X_monthly_train, X_monthly_test, y_monthly_train, y_monthly_test = train_test_split(
    monthly_features, monthly_target, test_size=0.2, random_state=42)

# Train a regression model
from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_monthly_train, y_monthly_train)

# Predict and evaluate
monthly_predictions = rf_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly Prediction MAE: {mae_monthly:.2f}")


#==============


# Import required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load the datasets (replace with your actual file paths)
ais_data = pd.read_csv("ais_data.csv")
crude_prices = pd.read_csv("crude_prices.csv")
market_data = pd.read_csv("market_data.csv")
supply_demand = pd.read_csv("supply_demand.csv")

# Merge datasets on the 'date' column
data = pd.merge(ais_data, crude_prices, on="date")
data = pd.merge(data, market_data, on="date")
data = pd.merge(data, supply_demand, on="date")

# Convert 'date' to datetime
data['date'] = pd.to_datetime(data['date'])

# Create month-year column for aggregation purposes
data['month'] = data['date'].dt.to_period('M')

# ==========================
# Feature Engineering
# ==========================

# Create lag features
data['brent_price_lag1'] = data['brent_price'].shift(1)
data['laden_perc_7day_avg'] = data['laden_perc'].rolling(window=7).mean()

# Add cumulative count of VLCCs within a month
data['cum_count_vlcc'] = data.groupby('month')['Count_VLCC'].cumsum()

# Add scenario feature for situational analysis
data['Exp_TD3C_KBD_scenario'] = data['Exp_TD3C_KBD'] + 500

# Drop rows with NaN after feature creation
data = data.dropna()

# ==========================
# Preparing Data for Daily Predictions
# ==========================
# Define daily features and target
daily_features = data.drop(columns=['td_rate', 'date', 'month'])
daily_target = data['td_rate']

# Split data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(daily_features, daily_target, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# Reshape input for LSTM (3D input: [samples, timesteps, features])
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)
X_test_lstm = X_test_tensor.unsqueeze(1)

# ==========================
# Define LSTM Model
# ==========================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last time step output
        return output

# Initialize model, loss function, and optimizer
input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = 1

model = LSTMModel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# ==========================
# Train the LSTM Model
# ==========================
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        val_output = model(X_val_lstm)
        val_loss = criterion(val_output, y_val_tensor)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

# ==========================
# Evaluate Daily Prediction
# ==========================
model.eval()
with torch.no_grad():
    predictions = model(X_test_lstm).numpy()
mae_daily = mean_absolute_error(y_test, predictions)
rmse_daily = mean_squared_error(y_test, predictions, squared=False)
print(f"Daily Prediction MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}")

# ==========================
# Preparing Data for Monthly Predictions
# ==========================
monthly_data = data.groupby('month').agg({
    'brent_price': 'mean',
    'laden_perc': 'mean',
    'cum_count_vlcc': 'max',  # Use total VLCC count
    'td_rate': 'mean'  # Target for monthly prediction
}).reset_index()

monthly_features = monthly_data.drop(columns=['td_rate', 'month'])
monthly_target = monthly_data['td_rate']

X_monthly_train, X_monthly_test, y_monthly_train, y_monthly_test = train_test_split(
    monthly_features, monthly_target, test_size=0.2, random_state=42)

# Train a simple regression model for monthly predictions
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_monthly_train, y_monthly_train)

# Predict and evaluate
monthly_predictions = rf_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly Prediction MAE: {mae_monthly:.2f}")

# ==========================
# Scenario Analysis
# ==========================
scenario_data = data.copy()
scenario_data['Exp_TD3C_KBD'] += 1500
scenario_features = scaler.transform(scenario_data.drop(columns=['td_rate', 'date', 'month']))
scenario_features_tensor = torch.tensor(scenario_features, dtype=torch.float32).unsqueeze(1)

# Predict scenario
model.eval()
with torch.no_grad():
    scenario_predictions = model(scenario_features_tensor).numpy()

# Compute range
scenario_lower = np.percentile(scenario_predictions, 2.5)
scenario_upper = np.percentile(scenario_predictions, 97.5)
print(f"Scenario Price Range: {scenario_lower:.2f} - {scenario_upper:.2f}")




#==============================================================


import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Load your data
ais_data = pd.read_csv("ais_data.csv")
crude_prices = pd.read_csv("crude_prices.csv")
market_data = pd.read_csv("market_data.csv")
supply_demand = pd.read_csv("supply_demand.csv")

# Merge datasets on date
data = pd.merge(ais_data, crude_prices, on="date")
data = pd.merge(data, market_data, on="date")
data = pd.merge(data, supply_demand, on="date")


# Example feature engineering
data['brent_price_lag1'] = data['brent_price'].shift(1)
data['laden_perc_7day_avg'] = data['laden_perc'].rolling(window=7).mean()
data['Exp_TD3C_KBD_scenario'] = data['Exp_TD3C_KBD'] + 500

# Drop rows with NaN after feature creation
data = data.dropna()

# Define target and features
features = data.drop(columns=['td_rate', 'date'])
target = data['td_rate']


# Split data
X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Convert to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)


class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last time step output
        return output


# Initialize model, loss, and optimizer
input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = 1

model = LSTMModel(input_dim, hidden_dim, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Reshape input for LSTM (3D input: [samples, timesteps, features])
X_train_lstm = X_train_tensor.unsqueeze(1)
X_val_lstm = X_val_tensor.unsqueeze(1)

# Training loop
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_lstm)
    loss = criterion(output, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    # Validation
    model.eval()
    val_output = model(X_val_lstm)
    val_loss = criterion(val_output, y_val_tensor)
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")


# Reshape test data
X_test_lstm = X_test_tensor.unsqueeze(1)

# Predict and evaluate
model.eval()
predictions = model(X_test_lstm).detach().numpy()
mae = mean_absolute_error(y_test, predictions)
rmse = mean_squared_error(y_test, predictions, squared=False)
print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}")


# Aggregate to monthly
data['month'] = pd.to_datetime(data['date']).dt.to_period('M')
monthly_data = data.groupby('month').mean().reset_index()

# Prepare monthly features
monthly_features = monthly_data.drop(columns=['td_rate', 'month'])
monthly_target = monthly_data['td_rate']

X_monthly_train, X_monthly_test, y_monthly_train, y_monthly_test = train_test_split(
    monthly_features, monthly_target, test_size=0.2, random_state=42)

# Train a simple linear regression for monthly predictions
from sklearn.linear_model import LinearRegression
lr_model = LinearRegression()
lr_model.fit(X_monthly_train, y_monthly_train)

# Predict and evaluate
monthly_predictions = lr_model.predict(X_monthly_test)
mae_monthly = mean_absolute_error(y_monthly_test, monthly_predictions)
print(f"Monthly MAE: {mae_monthly:.2f}")


# Compute 95% confidence interval for daily predictions
lower_bound = np.percentile(predictions, 2.5)
upper_bound = np.percentile(predictions, 97.5)
print(f"Predicted daily price range: {lower_bound:.2f} - {upper_bound:.2f}")


# Simulate scenario
scenario_data = data.copy()
scenario_data['Exp_TD3C_KBD'] += 1500  # Example scenario
scenario_features = scaler.transform(scenario_data.drop(columns=['td_rate', 'date']))

# Predict for scenario
scenario_features_tensor = torch.tensor(scenario_features, dtype=torch.float32).unsqueeze(1)
scenario_predictions = model(scenario_features_tensor).detach().numpy()

# Compute range for the scenario
scenario_lower = np.percentile(scenario_predictions, 2.5)
scenario_upper = np.percentile(scenario_predictions, 97.5)
print(f"Scenario price range: {scenario_lower:.2f} - {scenario_upper:.2f}")


