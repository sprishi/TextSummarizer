Your approach to forecasting vessel counts using both ARIMA and VAR models is reasonable, especially given the potential for both independence and interdependence among the vessel types. Let's break down your approach and then provide code for both methods.

Approach Analysis:
1. Auto ARIMA (using pmdarima): This is a good choice for univariate time series forecasting. It will model each vessel type independently, which is useful if the counts are not strongly correlated.

2. Vector Autoregression (VAR): This multivariate approach can capture interdependencies between the vessel types, which is valuable if there are significant relationships among them.

Using both methods allows you to compare results and potentially gain insights from both perspectives. This approach is correct and can provide a comprehensive analysis of your data.

Now, let's write the code for both approaches:



```python
import pandas as pd
import numpy as np
from pmdarima import auto_arima
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error
from math import sqrt

# Load the data
data = pd.read_csv('vessel_counts.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)

# Separate the vessel types
aframax = data['Aframax']
vlcc = data['VLCC']
suezmax = data['Suezmax']

# Function to check stationarity
def check_stationarity(timeseries):
    result = adfuller(timeseries)
    return result[1] <= 0.05

# Function to make a series stationary
def make_stationary(timeseries):
    diff = 0
    while not check_stationarity(timeseries):
        timeseries = timeseries.diff().dropna()
        diff += 1
    return timeseries, diff

# Auto ARIMA Forecasting
def arima_forecast(series, name, forecast_horizon=8):
    model = auto_arima(series, seasonal=False, stepwise=True, suppress_warnings=True)
    forecast = model.predict(n_periods=forecast_horizon)
    return pd.Series(forecast, name=name)

# VAR Forecasting
def var_forecast(data, forecast_horizon=8):
    # Make series stationary
    stationary_data = pd.DataFrame()
    diff_orders = {}
    for column in data.columns:
        stationary_series, diff = make_stationary(data[column])
        stationary_data[column] = stationary_series
        diff_orders[column] = diff
    
    # Fit VAR model
    model = VAR(stationary_data)
    results = model.fit()
    
    # Forecast
    forecast = results.forecast(stationary_data.values, steps=forecast_horizon)
    
    # Invert differencing
    for i, column in enumerate(data.columns):
        for _ in range(diff_orders[column]):
            forecast[:, i] = np.r_[data[column].iloc[-1], forecast[:, i]].cumsum()[1:]
    
    return pd.DataFrame(forecast, columns=data.columns, index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W'))

# Perform forecasts
arima_forecasts = pd.DataFrame({
    'Aframax': arima_forecast(aframax, 'Aframax'),
    'VLCC': arima_forecast(vlcc, 'VLCC'),
    'Suezmax': arima_forecast(suezmax, 'Suezmax')
})

var_forecasts = var_forecast(data)

# Print results
print("ARIMA Forecasts:")
print(arima_forecasts)
print("\nVAR Forecasts:")
print(var_forecasts)

# Calculate RMSE for both methods (assuming last 8 weeks are held out for validation)
actual_values = data.iloc[-8:]

arima_rmse = sqrt(mean_squared_error(actual_values, arima_forecasts))
var_rmse = sqrt(mean_squared_error(actual_values, var_forecasts))

print(f"\nARIMA RMSE: {arima_rmse}")
print(f"VAR RMSE: {var_rmse}")

```

This code implements both the Auto ARIMA and VAR approaches for forecasting your vessel counts. Here's a breakdown of what the code does:

1. It loads the data from a CSV file (you'll need to replace 'vessel_counts.csv' with your actual file name).
2. For the Auto ARIMA approach, it uses pmdarima to forecast each vessel type independently.
3. For the VAR approach, it first checks for stationarity and differences the data if necessary. Then it fits a VAR model and produces forecasts.
4. Both forecasts are printed, and RMSE is calculated for both methods (assuming you have 8 weeks of held-out data for validation).

To use this code, you'll need to:

1. Install the required libraries: pandas, numpy, pmdarima, statsmodels, sklearn.
2. Prepare your data in a CSV file with columns: Date, Aframax, VLCC, Suezmax.
3. Adjust the forecast horizon if needed (currently set to 8 weeks).

A few notes:

- The code assumes weekly data. If your data has a different frequency, you'll need to adjust the date range in the VAR forecast function.
- The VAR model implementation includes differencing to ensure stationarity. This is important for the model's validity, but it might affect interpretability.
- You might want to experiment with including or excluding seasonality in the ARIMA models, depending on your data's characteristics.
- Consider plotting the forecasts alongside historical data for visual comparison.

Remember to interpret the results carefully. If the VAR model performs significantly better, it might indicate strong interdependencies between vessel types. If ARIMA performs better, it might suggest that the vessel types are more independent than expected.

Would you like me to explain any part of the code in more detail?


2 =====================================



Certainly. Backtesting is an excellent approach to validate our models before using them for future forecasts. We'll modify the code to include backtesting, which will help us compare the performance of both ARIMA and VAR models on historical data. We'll split the data into training and testing sets, fit our models on the training data, and then evaluate their performance on the test data.

Let's update our approach:

1. We'll use data from 2021 Jan to 2024 Jun (as you mentioned), but we'll split it into training and testing sets.
2. We'll use an 80-20 split ratio, which is common in time series forecasting. This means we'll use about 80% of our data for training and 20% for testing.
3. We'll implement backtesting for both ARIMA and VAR models.
4. We'll compare the results using metrics like RMSE (Root Mean Square Error) and MAPE (Mean Absolute Percentage Error).
5. Based on these results, we'll choose the better performing model for future forecasts.

Here's the updated code incorporating these changes:



```python
import pandas as pd
import numpy as np
from pmdarima import auto_arima
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from math import sqrt

# Load the data
data = pd.read_csv('vessel_counts.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)

# Split the data into training and testing sets (80-20 split)
train_size = int(len(data) * 0.8)
train_data = data[:train_size]
test_data = data[train_size:]

# Function to check stationarity
def check_stationarity(timeseries):
    result = adfuller(timeseries)
    return result[1] <= 0.05

# Function to make a series stationary
def make_stationary(timeseries):
    diff = 0
    while not check_stationarity(timeseries):
        timeseries = timeseries.diff().dropna()
        diff += 1
    return timeseries, diff

# Auto ARIMA Forecasting
def arima_forecast(train, test, name):
    model = auto_arima(train, seasonal=False, stepwise=True, suppress_warnings=True)
    forecast = model.predict(n_periods=len(test))
    return pd.Series(forecast, index=test.index, name=name)

# VAR Forecasting
def var_forecast(train, test):
    # Make series stationary
    stationary_data = pd.DataFrame()
    diff_orders = {}
    for column in train.columns:
        stationary_series, diff = make_stationary(train[column])
        stationary_data[column] = stationary_series
        diff_orders[column] = diff
    
    # Fit VAR model
    model = VAR(stationary_data)
    results = model.fit()
    
    # Forecast
    forecast = results.forecast(stationary_data.values, steps=len(test))
    
    # Invert differencing
    for i, column in enumerate(train.columns):
        for _ in range(diff_orders[column]):
            forecast[:, i] = np.r_[train[column].iloc[-1], forecast[:, i]].cumsum()[1:]
    
    return pd.DataFrame(forecast, columns=train.columns, index=test.index)

# Perform backtesting
arima_forecasts = pd.DataFrame({
    'Aframax': arima_forecast(train_data['Aframax'], test_data['Aframax'], 'Aframax'),
    'VLCC': arima_forecast(train_data['VLCC'], test_data['VLCC'], 'VLCC'),
    'Suezmax': arima_forecast(train_data['Suezmax'], test_data['Suezmax'], 'Suezmax')
})

var_forecasts = var_forecast(train_data, test_data)

# Calculate metrics
def calculate_metrics(actual, predicted):
    rmse = sqrt(mean_squared_error(actual, predicted))
    mape = mean_absolute_percentage_error(actual, predicted)
    return rmse, mape

arima_metrics = {col: calculate_metrics(test_data[col], arima_forecasts[col]) for col in test_data.columns}
var_metrics = {col: calculate_metrics(test_data[col], var_forecasts[col]) for col in test_data.columns}

# Print results
print("Backtesting Results:")
print("\nARIMA Metrics:")
for col, (rmse, mape) in arima_metrics.items():
    print(f"{col} - RMSE: {rmse:.2f}, MAPE: {mape:.2%}")

print("\nVAR Metrics:")
for col, (rmse, mape) in var_metrics.items():
    print(f"{col} - RMSE: {rmse:.2f}, MAPE: {mape:.2%}")

# Determine the better model based on average RMSE
arima_avg_rmse = np.mean([metrics[0] for metrics in arima_metrics.values()])
var_avg_rmse = np.mean([metrics[0] for metrics in var_metrics.values()])

better_model = "ARIMA" if arima_avg_rmse < var_avg_rmse else "VAR"
print(f"\nBetter performing model based on average RMSE: {better_model}")

# Forecast future values using the better model
forecast_horizon = 8  # 8 weeks

if better_model == "ARIMA":
    future_forecasts = pd.DataFrame({
        'Aframax': arima_forecast(data['Aframax'], pd.Series(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')), 'Aframax'),
        'VLCC': arima_forecast(data['VLCC'], pd.Series(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')), 'VLCC'),
        'Suezmax': arima_forecast(data['Suezmax'], pd.Series(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')), 'Suezmax')
    })
else:
    future_forecasts = var_forecast(data, pd.DataFrame(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')))

print("\nFuture Forecasts:")
print(future_forecasts)

```

This updated code implements backtesting and chooses the better performing model for future forecasts. Here's a breakdown of the changes and how to interpret the results:

1. Data Splitting: The data is split into training (80%) and testing (20%) sets.

2. Backtesting: Both ARIMA and VAR models are trained on the training set and then used to forecast values for the test set period.

3. Metrics: We calculate RMSE (Root Mean Square Error) and MAPE (Mean Absolute Percentage Error) for each vessel type for both models. 
   - RMSE measures the standard deviation of the residuals (prediction errors)
   - MAPE provides a percentage measure of error, which is often easier to interpret

4. Model Selection: The code determines the better performing model based on the average RMSE across all vessel types.

5. Future Forecasting: Using the better performing model, we then forecast the next 8 weeks of vessel counts.

To use this code:

1. Ensure your CSV file ('vessel_counts.csv') contains data from 2021 Jan to 2024 Jun, with columns: Date, Aframax, VLCC, Suezmax.
2. Run the code to see the backtesting results and future forecasts.

Interpreting the results:

1. Look at the RMSE and MAPE for each vessel type under both ARIMA and VAR models. Lower values indicate better performance.
2. The code will tell you which model performed better on average based on RMSE.
3. Check the future forecasts to see the predicted vessel counts for the next 8 weeks.

Additional considerations:

1. If one model performs significantly better for certain vessel types, you might consider using different models for different vessel types.
2. You may want to visualize the backtesting results and future forecasts using plots for easier interpretation.
3. Consider the nature of your data. If there are strong interdependencies between vessel types, VAR might be more appropriate. If each type seems to follow its own pattern, ARIMA might work better.

Would you like me to explain any part of this process further or add any additional features to the analysis?



3 ==================================



Absolutely! You're right to consider hyperparameter tuning and cross-validation for time series forecasting. For both Auto ARIMA and VAR models, we can implement techniques similar to cross-validation used in traditional machine learning. In time series, we typically use a method called "time series cross-validation" or "rolling forecast origin" technique.

For Auto ARIMA, we don't need to manually tune hyperparameters as it automatically selects the best parameters. However, we can make it more robust by allowing it to consider seasonal components and by using time series cross-validation to evaluate its performance.

For VAR, we can use time series cross-validation to select the optimal order (number of lags) of the model.

Let's update our code to incorporate these improvements:



```python
import pandas as pd
import numpy as np
from pmdarima import auto_arima
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
from math import sqrt

# Load the data
data = pd.read_csv('vessel_counts.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)

# Function to check stationarity
def check_stationarity(timeseries):
    result = adfuller(timeseries)
    return result[1] <= 0.05

# Function to make a series stationary
def make_stationary(timeseries):
    diff = 0
    while not check_stationarity(timeseries):
        timeseries = timeseries.diff().dropna()
        diff += 1
    return timeseries, diff

# Time series cross-validation for ARIMA
def ts_cv_arima(data, n_splits=5, initial=52, horizon=8):
    errors = []
    for i in range(n_splits):
        train = data[:-(horizon*(n_splits-i))]
        test = data[-(horizon*(n_splits-i)):-horizon*(n_splits-i-1)]
        model = auto_arima(train, seasonal=True, m=52, stepwise=True, suppress_warnings=True)
        forecast = model.predict(n_periods=horizon)
        error = mean_squared_error(test, forecast, squared=False)
        errors.append(error)
    return np.mean(errors)

# Time series cross-validation for VAR
def ts_cv_var(data, max_order=10, n_splits=5, initial=52, horizon=8):
    errors = {i: [] for i in range(1, max_order+1)}
    for i in range(n_splits):
        train = data[:-(horizon*(n_splits-i))]
        test = data[-(horizon*(n_splits-i)):-horizon*(n_splits-i-1)]
        for order in range(1, max_order+1):
            model = VAR(train)
            results = model.fit(order)
            forecast = results.forecast(train.values[-order:], steps=horizon)
            error = mean_squared_error(test, forecast, squared=False)
            errors[order].append(error)
    return {order: np.mean(error_list) for order, error_list in errors.items()}

# Robust ARIMA Forecasting
def robust_arima_forecast(train, test, name):
    model = auto_arima(train, seasonal=True, m=52, stepwise=True, suppress_warnings=True)
    forecast = model.predict(n_periods=len(test))
    return pd.Series(forecast, index=test.index, name=name)

# Robust VAR Forecasting
def robust_var_forecast(train, test):
    # Make series stationary
    stationary_data = pd.DataFrame()
    diff_orders = {}
    for column in train.columns:
        stationary_series, diff = make_stationary(train[column])
        stationary_data[column] = stationary_series
        diff_orders[column] = diff
    
    # Find optimal order
    var_errors = ts_cv_var(stationary_data)
    optimal_order = min(var_errors, key=var_errors.get)
    
    # Fit VAR model with optimal order
    model = VAR(stationary_data)
    results = model.fit(optimal_order)
    
    # Forecast
    forecast = results.forecast(stationary_data.values[-optimal_order:], steps=len(test))
    
    # Invert differencing
    for i, column in enumerate(train.columns):
        for _ in range(diff_orders[column]):
            forecast[:, i] = np.r_[train[column].iloc[-1], forecast[:, i]].cumsum()[1:]
    
    return pd.DataFrame(forecast, columns=train.columns, index=test.index)

# Split the data into training and testing sets (80-20 split)
train_size = int(len(data) * 0.8)
train_data = data[:train_size]
test_data = data[train_size:]

# Perform cross-validation
print("Performing cross-validation...")
arima_cv_errors = {
    'Aframax': ts_cv_arima(train_data['Aframax']),
    'VLCC': ts_cv_arima(train_data['VLCC']),
    'Suezmax': ts_cv_arima(train_data['Suezmax'])
}
var_cv_errors = ts_cv_var(train_data)

print("ARIMA CV Errors:", arima_cv_errors)
print("VAR CV Errors:", var_cv_errors)

# Perform forecasting on test set
print("\nForecasting on test set...")
arima_forecasts = pd.DataFrame({
    'Aframax': robust_arima_forecast(train_data['Aframax'], test_data['Aframax'], 'Aframax'),
    'VLCC': robust_arima_forecast(train_data['VLCC'], test_data['VLCC'], 'VLCC'),
    'Suezmax': robust_arima_forecast(train_data['Suezmax'], test_data['Suezmax'], 'Suezmax')
})

var_forecasts = robust_var_forecast(train_data, test_data)

# Calculate metrics
def calculate_metrics(actual, predicted):
    rmse = sqrt(mean_squared_error(actual, predicted))
    mape = mean_absolute_percentage_error(actual, predicted)
    return rmse, mape

arima_metrics = {col: calculate_metrics(test_data[col], arima_forecasts[col]) for col in test_data.columns}
var_metrics = {col: calculate_metrics(test_data[col], var_forecasts[col]) for col in test_data.columns}

# Print results
print("\nTest Set Results:")
print("\nARIMA Metrics:")
for col, (rmse, mape) in arima_metrics.items():
    print(f"{col} - RMSE: {rmse:.2f}, MAPE: {mape:.2%}")

print("\nVAR Metrics:")
for col, (rmse, mape) in var_metrics.items():
    print(f"{col} - RMSE: {rmse:.2f}, MAPE: {mape:.2%}")

# Determine the better model based on average RMSE
arima_avg_rmse = np.mean([metrics[0] for metrics in arima_metrics.values()])
var_avg_rmse = np.mean([metrics[0] for metrics in var_metrics.values()])

better_model = "ARIMA" if arima_avg_rmse < var_avg_rmse else "VAR"
print(f"\nBetter performing model based on average RMSE: {better_model}")

# Forecast future values using the better model
forecast_horizon = 8  # 8 weeks

if better_model == "ARIMA":
    future_forecasts = pd.DataFrame({
        'Aframax': robust_arima_forecast(data['Aframax'], pd.Series(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')), 'Aframax'),
        'VLCC': robust_arima_forecast(data['VLCC'], pd.Series(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')), 'VLCC'),
        'Suezmax': robust_arima_forecast(data['Suezmax'], pd.Series(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')), 'Suezmax')
    })
else:
    future_forecasts = robust_var_forecast(data, pd.DataFrame(index=pd.date_range(start=data.index[-1] + pd.Timedelta(days=7), periods=forecast_horizon, freq='W')))

print("\nFuture Forecasts:")
print(future_forecasts)

```

This updated code incorporates time series cross-validation and makes both ARIMA and VAR models more robust. Here's a breakdown of the changes:

1. Time Series Cross-Validation:
   - For ARIMA: We use a rolling forecast origin technique. The function `ts_cv_arima` splits the training data into multiple train-test sets and evaluates the model's performance on each.
   - For VAR: The function `ts_cv_var` does the same, but also tests different orders (lags) of the VAR model to find the optimal one.

2. Robust ARIMA:
   - We now allow for seasonal components (m=52 for weekly data) in the `auto_arima` function.
   - The `robust_arima_forecast` function uses these settings for the final model.

3. Robust VAR:
   - We use the optimal order found during cross-validation in the `robust_var_forecast` function.

4. Model Evaluation:
   - We first perform cross-validation on the training set to get an idea of each model's performance.
   - We then forecast on the test set and calculate RMSE and MAPE for a final comparison.

5. Future Forecasting:
   - We use the better performing model (based on test set RMSE) to forecast the next 8 weeks.

To use this code:

1. Ensure your CSV file ('vessel_counts.csv') contains weekly data from 2021 Jan to 2024 Jun, with columns: Date, Aframax, VLCC, Suezmax.
2. Run the code to see the cross-validation results, test set performance, and future forecasts.

Interpreting the results:

1. Cross-validation errors give you an idea of each model's performance on the training data.
2. Test set metrics (RMSE and MAPE) show you how well each model performs on unseen data.
3. The code selects the better performing model based on average RMSE on the test set.
4. Future forecasts are made using the better performing model.

Additional considerations:

1. The code assumes weekly data with potential yearly seasonality (m=52). If your data has a different frequency or seasonality, adjust these parameters accordingly.
2. The cross-validation process can be computationally intensive, especially for large datasets. You might need to adjust the number of splits or the maximum VAR order if it takes too long to run.
3. Remember that while this approach is more robust, all forecasts come with uncertainty. It's often useful to provide confidence intervals with point forecasts, which could be a future enhancement to this code.

Would you like me to explain any part of this process further or add any additional features to the analysis?
