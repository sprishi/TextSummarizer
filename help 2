
import pandas as pd

def update_top_locations(group):
    # Initialize a list to hold the top locations for each row
    top_locs = []
    # Initialize a dictionary to hold the frequency of each location
    freq = {}
    
    # Iterate over each load country in the group
    for country in group['load_en']:
        # Update the frequency dictionary
        freq[country] = freq.get(country, 0) + 1
        # Create a sorted list of countries by their frequency
        sorted_countries = sorted(freq, key=freq.get, reverse=True)
        # Append the top 5 countries to the top_locs list, using None if there are less than 5
        top_locs.append(sorted_countries[:5] + [None]*(5-len(sorted_countries)))
    
    # Convert the list of top locations into a DataFrame
    top_locs_df = pd.DataFrame(top_locs, columns=[f'top_loc_{i}' for i in range(1, 6)])
    # Shift the data down by one row so the current load country affects the next row
    group = group.join(top_locs_df.shift(1))
    return group

# Apply the function to each flight group
updated_df = data_df.groupby('flight_no').apply(update_top_locations).reset_index(drop=True)



------------- 

# Assuming df is your entire dataset and df_new is your unseen data
df_combined = pd.concat([df, df_new])

# Function to get the top countries for each flight
def get_top_countries(group):
    top_countries = group['load_country'].value_counts().index.tolist()
    next_idx = group.index[-1] + 1
    if next_idx in df_combined.index:  # Check if there's a next row
        for i in range(5):
            if i < len(top_countries):
                df_combined.at[next_idx, f'top_country_{i+1}'] = top_countries[i]
            else:
                df_combined.at[next_idx, f'top_country_{i+1}'] = None

df_combined.groupby('no').apply(get_top_countries)





---------------


import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras import backend as K

# Custom F1 score metric
def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# Prepare sequences
def create_sequences(X, y, unique_nos, take_last_n=1):
    X_list = []
    y_list = []
    for flight_no in unique_nos:
        flight_data = X[X['no'] == flight_no]
        X_list.append(flight_data.drop('no', axis=1).values)  # assuming 'no' is the flight number column
        y_list.append(y.loc[flight_data.index][-take_last_n])  # get only the last value for y
    return X_list, y_list

# Splitting the data into train and validation based on the last occurrence
unique_flight_numbers = X_train['no'].unique()
validation_flight_numbers = [X_train[X_train['no'] == no].iloc[-1] for no in unique_flight_numbers]
validation_indices = [index for index, _ in validation_flight_numbers]
X_val = X_train.loc[validation_indices]
y_val = y_train.loc[validation_indices]
X_train = X_train.drop(validation_indices)
y_train = y_train.drop(validation_indices)

# Creating sequences for training, validation, and test sets
X_train_list, y_train_list = create_sequences(X_train, y_train, X_train['no'].unique())
X_val_list, y_val_list = create_sequences(X_val, y_val, X_val['no'].unique())
X_test_list, y_test_list = create_sequences(X_test, y_test, X_test['no'].unique())

# Padding sequences
X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_list, padding='post', dtype='float32')
y_train_padded = np.array(y_train_list)
X_val_padded = tf.keras.preprocessing.sequence.pad_sequences(X_val_list, padding='post', dtype='float32')
y_val_padded = np.array(y_val_list)
X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_list, padding='post', dtype='float32')
y_test_padded = np.array(y_test_list)

# Define LSTM model
model = Sequential()
model.add(Masking(mask_value=0., input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])))  # Masking layer for padded values
model.add(LSTM(50, return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(50))
model.add(Dropout(0.1))
model.add(Dense(len(np.unique(y_train)), activation='softmax'))

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[f1])

# Train model
history = model.fit(X_train_padded, y_train_padded, epochs=50, batch_size=32, verbose=1,
                    validation_data=(X_val_padded, y_val_padded), callbacks=[early_stopping])

# Prediction and Evaluation on test set
y_pred = model.predict(X_test_padded)
y_pred_classes = np.argmax(y_pred, axis=1)

print("Accuracy on test set:", accuracy_score(y_test_padded, y_pred_classes))
print("F1 Score on test set:", f1_score(y_test_padded, y_pred_classes, average='macro'))





