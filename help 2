# Assuming df is your entire dataset and df_new is your unseen data
df_combined = pd.concat([df, df_new])

# Function to get the top countries for each flight
def get_top_countries(group):
    top_countries = group['load_country'].value_counts().index.tolist()
    next_idx = group.index[-1] + 1
    if next_idx in df_combined.index:  # Check if there's a next row
        for i in range(5):
            if i < len(top_countries):
                df_combined.at[next_idx, f'top_country_{i+1}'] = top_countries[i]
            else:
                df_combined.at[next_idx, f'top_country_{i+1}'] = None

df_combined.groupby('no').apply(get_top_countries)





---------------

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Masking
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from sklearn.metrics import f1_score, accuracy_score
from keras import backend as K

# Custom F1 Score metric
def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# Splitting data to train and validation sets
train_indices = int(0.9 * X_train_padded.shape[0])
X_val_padded = X_train_padded[train_indices:]
y_val_padded = y_train_padded[train_indices:]
X_train_padded = X_train_padded[:train_indices]
y_train_padded = y_train_padded[:train_indices]

# LSTM Model
model = Sequential()
model.add(Masking(mask_value=0, input_shape=(None, X_train_padded.shape[2])))
model.add(LSTM(100, return_sequences=True, kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(LSTM(50, kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(y_encoded.max()+1, activation='softmax'))

# Optimizer
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy', f1])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10)

# Model Training
history = model.fit(X_train_padded, y_train_padded, epochs=500, batch_size=32, 
                    validation_data=(X_val_padded, y_val_padded), callbacks=[early_stopping], verbose=1)

# Predictions
y_pred = model.predict(X_test_padded)
y_pred_labels = np.argmax(y_pred, axis=1)

# Evaluation
accuracy = accuracy_score(y_test_padded, y_pred_labels)
f1 = f1_score(y_test_padded, y_pred_labels, average='macro')

print("Accuracy:", accuracy)
print("F1 Score:", f1)



