
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def read_excel(file_path):
    """Read the Excel file and return a pandas DataFrame."""
    return pd.read_excel(file_path)

def create_charts(df, date_column, chart_configs):
    """
    Create multiple line charts based on the provided configurations.
    
    :param df: pandas DataFrame containing the data
    :param date_column: name of the column to be used as x-axis (dates)
    :param chart_configs: list of dictionaries containing chart configurations
    :return: plotly Figure object
    """
    num_charts = len(chart_configs)
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[config['chart_name'] for config in chart_configs],
        vertical_spacing=0.2,
        horizontal_spacing=0.1
    )
    
    for i, config in enumerate(chart_configs):
        row = i // 2 + 1
        col = i % 2 + 1
        
        # Filter data based on start_date if provided
        start_date = config.get('start_date')
        chart_df = df[df[date_column] >= start_date] if start_date else df
        
        for y_column in config['y_columns']:
            y_values = chart_df[y_column]
            
            # Apply scaling factor if provided for this column
            scale_factor = config.get('scale_factors', {}).get(y_column, 1)
            y_values = y_values / scale_factor
            
            fig.add_trace(
                go.Scatter(
                    x=chart_df[date_column], 
                    y=y_values, 
                    name=y_column,
                    legendgroup=f"group{i}",
                    legendgrouptitle_text=config['chart_name'],
                    showlegend=True
                ),
                row=row, col=col
            )
        
        fig.update_xaxes(title_text=date_column, row=row, col=col)
        fig.update_yaxes(title_text=config['y_axis_label'], row=row, col=col)

    # Update layout
    fig.update_layout(
        height=1000,
        width=1200, 
        title_text="Dynamic Multi-Chart Dashboard",
        legend=dict(
            groupclick="toggleitem"
        )
    )

    return fig

def main():
    # Example usage
    file_path = "your_excel_file.xlsx"
    df = read_excel(file_path)
    
    date_column = "Date"
    chart_configs = [
        {
            'chart_name': 'Chart 1 (from 2018)',
            'y_columns': ['Column1', 'Column2', 'Column3'],
            'y_axis_label': 'Values',
            'start_date': '2018-01-01'
        },
        {
            'chart_name': 'Chart 2',
            'y_columns': ['Column4', 'Column5'],
            'y_axis_label': 'Metrics'
        },
        {
            'chart_name': 'Chart 3',
            'y_columns': ['Column6'],
            'y_axis_label': 'Score'
        },
        {
            'chart_name': 'Chart 4 (Scaled)',
            'y_columns': ['Column7', 'Column8', 'Column9', 'Column10'],
            'y_axis_label': 'Performance',
            'scale_factors': {'Column7': 100, 'Column8': 10}  # Only Column7 and Column8 are scaled
        }
    ]
    
    fig = create_charts(df, date_column, chart_configs)
    fig.show()

if __name__ == "__main__":
    main()

----------------------

import streamlit as st
from pages import home, admin_dashboard, user_management, profile, reports, about

# Define all pages
all_pages = {
    'home': ('Home', home.show),
    'admin_dashboard': ('Admin Dashboard', admin_dashboard.show),
    'user_management': ('User Management', user_management.show),
    'profile': ('Profile', profile.show),
    'reports': ('Reports', reports.show),
    'about': ('About', about.show)
}

# Define user roles and their accessible pages
user_roles = {
    'admin': ['home', 'admin_dashboard', 'user_management'],
    'user': ['home', 'profile', 'reports'],
    'guest': ['home', 'about']
}

def login(username, password):
    # Replace this with your actual authentication logic
    if username == 'admin' and password == 'admin_pass':
        return 'admin'
    elif username == 'user' and password == 'user_pass':
        return 'user'
    return None

def show_navigation(role):
    st.sidebar.title("Navigation")
    for page_key in user_roles[role]:
        page_name, _ = all_pages[page_key]
        if st.sidebar.button(page_name, key=f"nav_{page_key}"):
            st.session_state.current_page = page_key

def main():
    if 'logged_in' not in st.session_state:
        st.session_state.logged_in = False
    
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 'home'

    if not st.session_state.logged_in:
        username = st.text_input("Username")
        password = st.text_input("Password", type="password")
        if st.button("Login"):
            role = login(username, password)
            if role:
                st.session_state.logged_in = True
                st.session_state.role = role
                st.success("Logged in successfully!")
                st.experimental_rerun()
            else:
                st.error("Invalid username or password")
    else:
        show_navigation(st.session_state.role)
        
        # Display the current page
        page_name, page_function = all_pages[st.session_state.current_page]
        page_function()
        
        if st.sidebar.button("Logout"):
            st.session_state.logged_in = False
            st.session_state.role = None
            st.session_state.current_page = 'home'
            st.experimental_rerun()

if __name__ == "__main__":
    main()


====================================

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Assuming your DataFrame is named 'df'
# Ensure 'Report Date' is in datetime format
df_vessel = df_vessel.sort_values('Report Date')
    
    # Group by Report Date and calculate cumulative counts for each date
    daily_counts = df_vessel.groupby('Report Date').agg({
        'IMO': 'count',
        'Scrubber Fitted': lambda x: (x == 'Yes').sum(),
        'Destination Zone': lambda x: ((x == 'USG') & (df_vessel.loc[x.index, 'Availability'] == 15)).sum(),
        'Free Capacity': 'sum'
    }).reset_index()
    
    daily_counts.columns = ['Report Date', 'Cumulative Count', 'Cumulative Scrubber', 'Cumulative USG 15', 'Cumulative Free Capacity']
    
    # Calculate Cumulative USG 30
    daily_counts['Cumulative USG 30'] = df_vessel.groupby('Report Date').apply(
        lambda x: ((x['Destination Zone'] == 'USG') & (x['Availability'] == 30)).sum()
    ).values
    
    return daily_counts

# Prepare data for each vessel type
df_afra = prepare_data(df, 'Aframax')
df_suez = prepare_data(df, 'Suezmax')
df_vlcc = prepare_data(df, 'VLCC')
df_all = prepare_data(df, None)  # All vessel types

# Create the figure
fig = make_subplots(specs=[[{"secondary_y": True}]])

def add_traces(fig, df, name):
    fig.add_trace(go.Scatter(x=df['Report Date'], y=df['Cumulative Count'], name=f'{name} Total', line=dict(color='blue'), visible=name=='All'))
    fig.add_trace(go.Scatter(x=df['Report Date'], y=df['Cumulative Scrubber'], name=f'{name} Scrubber', line=dict(color='green'), visible=name=='All'))
    fig.add_trace(go.Scatter(x=df['Report Date'], y=df['Cumulative USG 15'], name=f'{name} USG 15', line=dict(color='red'), visible=name=='All'))
    fig.add_trace(go.Scatter(x=df['Report Date'], y=df['Cumulative USG 30'], name=f'{name} USG 30', line=dict(color='orange'), visible=name=='All'))
    fig.add_trace(go.Bar(x=df['Report Date'], y=df['Cumulative Free Capacity'], name=f'{name} Free Capacity', opacity=0.3, visible=name=='All'), secondary_y=True)

# Add traces for each vessel type and all combined
add_traces(fig, df_all, 'All')
add_traces(fig, df_afra, 'Aframax')
add_traces(fig, df_suez, 'Suezmax')
add_traces(fig, df_vlcc, 'VLCC')

# Function to update trace visibility and names
def update_traces(vessel_type):
    visibility = [False] * len(fig.data)
    new_names = []
    for i, trace in enumerate(fig.data):
        if vessel_type in trace.name:
            visibility[i] = True
            new_names.append(trace.name.replace(f"{vessel_type} ", ""))
        else:
            new_names.append(trace.name)
    return dict(visible=visibility), [dict(name=name) for name in new_names]

# Update layout
fig.update_layout(
    title='Vessel Data Analysis',
    xaxis_title='Report Date',
    yaxis_title='Count',
    yaxis2_title='Free Capacity (KT)',
    legend_title='Metrics',
    updatemenus=[
        dict(
            buttons=list([
                dict(label="All", method="update", 
                     args=update_traces('All')),
                dict(label="Aframax", method="update", 
                     args=update_traces('Aframax')),
                dict(label="Suezmax", method="update", 
                     args=update_traces('Suezmax')),
                dict(label="VLCC", method="update", 
                     args=update_traces('VLCC')),
            ]),
            direction="down",
            pad={"r": 10, "t": 10},
            showactive=True,
            x=0.1,
            xanchor="left",
            y=1.1,
            yanchor="top"
        ),
    ]
)

# Show the figure
fig.show()




# ------------------------------------------------


stacked = st.toggle('Stacked Bar Graph', value=True) if by_type else None

# Hide the second radio button if 'KBD' is selected
if show == "KBD":
    st.session_state.x_axis_by = "Date"
else:
    x_axis_by = st.session_state.x_axis_by

# Formatting functions
def format_week(x):
    return x.strftime('%d/%b')

def format_decade(month, decade):
    return f'{decade} {month.strftime("%b %y")}'

def format_month(x):
    return x.strftime('%b %y')

# Data manipulation for plotting
if show == "KBD":
    if by_type:
        fig = px.line(df_grouped, x='eta_date', y='kbd', color='type', title='KBD Over Time by Vessel Class', labels={'eta_date': 'Date', 'kbd': 'KBD'})
    else:
        fig = px.line(df_grouped, x='eta_date', y='kbd', title='KBD Over Time', labels={'eta_date': 'Date', 'kbd': 'KBD'})
elif show == "Count" or show == "Vol (KT)":
    if x_axis_by == "Week":
        df['week'] = df['eta'].dt.to_period('W').apply(lambda r: r.start_time.strftime('%d/%b'))
        if show == "Count":
            df_plot = df.groupby(['week', 'type' if by_type else None]).size().reset_index(name='count')
            y_col = 'count'
            title = 'Vessel Count by Week'
        else:
            df_plot = df.groupby(['week', 'type' if by_type else None])['volume_kt'].sum().reset_index()
            y_col = 'volume_kt'
            title = 'Vessel Volume by Week'
        if by_type:
            fig = px.bar(df_plot, x='week', y=y_col, color='type', title=title, text=y_col, barmode='stack' if stacked else 'group')
        else:
            fig = px.bar(df_plot, x='week', y=y_col, title=title, text=y_col)
    elif x_axis_by == "Decade":
        df['decade'] = df['eta'].dt.day.apply(lambda x: 'D1' if x <= 10 else 'D2' if x <= 20 else 'D3')
        df['month'] = df['eta'].dt.to_period('M').apply(lambda r: r.start_time)
        df['decade_formatted'] = df.apply(lambda row: format_decade(row['month'], row['decade']), axis=1)
        if show == "Count":
            df_plot = df.groupby(['decade_formatted', 'type' if by_type else None]).size().reset_index(name='count')
            y_col = 'count'
            title = 'Vessel Count by Decade'
        else:
            df_plot = df.groupby(['decade_formatted', 'type' if by_type else None])['volume_kt'].sum().reset_index()
            y_col = 'volume_kt'
            title = 'Vessel Volume by Decade'
        if by_type:
            fig = px.bar(df_plot, x='decade_formatted', y=y_col, color='type', title=title, text=y_col, barmode='stack' if stacked else 'group')
        else:
            fig = px.bar(df_plot, x='decade_formatted', y=y_col, title=title, text=y_col)
    elif x_axis_by == "Month":
        df['month'] = df['eta'].dt.to_period('M').apply(lambda r: r.start_time.strftime('%b %y'))
        if show == "Count":
            df_plot = df.groupby(['month', 'type' if by_type else None]).size().reset_index(name='count')
            y_col = 'count'
            title = 'Vessel Count by Month'
        else:
            df_plot = df.groupby(['month', 'type' if by_type else None])['volume_kt'].sum().reset_index()
            y_col = 'volume_kt'
            title = 'Vessel Volume by Month'
        if by_type:
            fig = px.bar(df_plot, x='month', y=y_col, color='type', title=title, text=y_col, barmode='stack' if stacked else 'group')
        else:
            fig = px.bar(df_plot, x='month', y=y_col, title=title, text=y_col)

# Add text annotations for bar graphs
if show != "KBD":
    fig.update_traces(textposition='outside', textfont=dict(color='black', size=12))

# Show the plot
st.plotly_chart(fig)



================================================================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, array, lit, struct, monotonically_increasing_id, min
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.appName("Efficient Cross Join in Batches").getOrCreate()

# Assuming ais_data (1.2M records) and ports_data (5K records) are already loaded as DataFrames

# Add unique ID to both DataFrames to ensure uniqueness
ais_data_with_id = ais_data.withColumn("ais_id", monotonically_increasing_id())
ports_data_with_id = ports_data.withColumn("port_id", monotonically_increasing_id())

# Define the UDF for distance calculation
# Ensure haversine_distance UDF is available

final_result_df = None

# Calculate the number of partitions based on desired batch size, e.g., 1000 records
num_partitions = int(ais_data_with_id.count() / 1000)

# Repartition ais_data_with_id for batch processing
ais_data_repartitioned = ais_data_with_id.repartition(num_partitions)

# Iterate through each partition (batch)
for i in range(num_partitions):
    # Filter the current batch
    current_batch = ais_data_repartitioned.filter(ais_data_repartitioned.rdd.id % num_partitions == i)
    
    # Explode the ports_data_with_id for cross join within the same DataFrame
    current_batch_cross_joined = current_batch.crossJoin(ports_data_with_id)
    
    # Calculate distances for each row in the current batch
    current_batch_with_distances = current_batch_cross_joined.withColumn(
        "distance",
        haversine_distance(
            col("ais_lat"), col("ais_lon"),
            col("port_lat"), col("port_lon")
        )
    )
    
    # Rank the results based on distance and take the closest port for each AIS record
    windowSpec = Window.partitionBy("ais_id").orderBy("distance")
    closest_ports = current_batch_with_distances.withColumn("rank", row_number().over(windowSpec)).filter(col("rank") == 1)
    
    # Append the result of the current batch to the final result DataFrame
    if final_result_df is None:
        final_result_df = closest_ports
    else:
        final_result_df = final_result_df.union(closest_ports)

# final_result_df now contains each AIS record paired with its closest port





---------------------------


from pyspark import SparkContext
from pyspark.sql import SQLContext
from sedona.utils import SedonaKryoRegistrator, KryoSerializer
from sedona.register import SedonaRegistrator
from dataiku import spark as dkuspark

# Initialize SparkContext and SQLContext
sc = SparkContext.getOrCreate()
sc.setSystemProperty("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
sc.setSystemProperty("spark.kryo.registrator", "org.apache.sedona.core.serde.SedonaKryoRegistrator")

sqlContext = SQLContext(sc)

# Register Sedona with the existing SQLContext
SedonaRegistrator.registerAll(sqlContext)

# Load datasets using Dataiku dkuspark
cleaned_ais_data = dkuspark.get_dataframe(sqlContext, "your_cleaned_ais_dataset_name")
ports_data = dkuspark.get_dataframe(sqlContext, "your_ports_dataset_name")

from sedona.register import SedonaRegistrator
from sedona.utils import KryoSerializer, SedonaKryoRegistrator
from sedona.core.formatMapper.shapefileParser import ShapefileReader
from sedona.core.SpatialRDD import SpatialRDD
from sedona.core.formatMapper import GeoJsonReader
from sedona.core.spatialOperator import JoinQuery
from sedona.sql.utils import Adapter
from sedona.core.geom.envelope import Envelope
from sedona.core.spatialOperator import KNNQuery
from sedona.core.spatialOperator import RangeQuery
from sedona.core.enums import FileDataSplitter
from sedona.core.enums import IndexType
from sedona.core.enums import JoinBuildSide
from sedona.core.spatialOperator import JoinQuery
from sedona.sql.utils import Adapter
from sedona.utils import SedonaKryoRegistrator, KryoSerializer

# Convert DataFrames to Spatial DataFrames by defining geometries
from sedona.sql.types import GeometryType
from pyspark.sql.functions import lit

# Assuming 'ais_lat' and 'ais_lon' are the latitude and longitude columns in your AIS data
cleaned_ais_data.createOrReplaceTempView("ais")
ports_data.createOrReplaceTempView("ports")

cleaned_ais_df = sqlContext.sql("""
SELECT *, st_point(cast(ais_lon as Decimal(24,20)), cast(ais_lat as Decimal(24,20))) as ais_geom 
FROM ais
""")

ports_df = sqlContext.sql("""
SELECT *, st_point(cast(port_lon as Decimal(24,20)), cast(port_lat as Decimal(24,20))) as port_geom 
FROM ports
""")

# Now you can use Sedona functions to perform spatial joins, range queries, etc.
# For example, to find the nearest port for each AIS record:
ais_with_nearest_port = Adapter.toDf(
    JoinQuery.DistanceJoinQueryFlat(cleaned_ais_df, ports_df, False, True),
    sqlContext
).selectExpr("leftgeometry as ais_geom", "rightgeometry as port_geom", "distance")

# Note: This is a basic example. You'll need to adjust field selections and operations according to your exact needs.

from pyspark.sql.window import Window
from pyspark.sql.functions import rank, col

# Assuming ais_with_nearest_port is the resulting DataFrame from the spatial join
# and it includes 'ais_id', 'port_id', and 'distance' columns among others

# Define a window specification to partition data by AIS record and order by distance
windowSpec = Window.partitionBy("ais_id").orderBy("distance")

# Apply the rank function within each partition (i.e., each AIS record's set of joined ports)
ranked_ports = ais_with_nearest_port.withColumn("rank", rank().over(windowSpec))

# Filter to keep only the top-ranked (nearest) port for each AIS record
nearest_ports = ranked_ports.filter(col("rank") == 1).drop("rank")

# nearest_ports now contains only the nearest port for each AIS record






----------------------------------


from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# Assuming 'train' and 'test' are your prepared datasets
X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]

# Define the model with initial parameters
model = XGBClassifier()  # Replace with your chosen model

# Walk-forward validation
n_splits = 10  # Number of splits
window_size = len(X_train) // n_splits  # Size of each training set

for i in range(1, n_splits + 1):
    split_size = window_size * i
    X_train_split = X_train[:split_size]
    y_train_split = y_train[:split_size]

    # Fit the model
    model.fit(X_train_split, y_train_split)

    # Evaluate on the next step
    y_pred = model.predict(X_test)
    print(f"Split {i}: Accuracy = {accuracy_score(y_test, y_pred)}, F1 Score = {f1_score(y_test, y_pred, average='macro')}")



--------------

from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
import lightgbm as lgb
from sklearn.metrics import accuracy_score, f1_score

# Assuming 'train' and 'test' are your prepared datasets
X_train = train[features]
X_test = test[features]
y_train = train[target]
y_test = test[target]

# TimeSeries Cross-validation
tscv = TimeSeriesSplit(n_splits=5)

# Parameter grid for LightGBM
param_grid = {
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [100, 200, 300],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

# LightGBM Classifier
lgb_model = lgb.LGBMClassifier()

# RandomizedSearch with TimeSeriesSplit
random_search_lgb = RandomizedSearchCV(
    estimator=lgb_model,
    param_distributions=param_grid,
    n_iter=100,  # Adjust based on computational resources
    scoring='accuracy',  # or 'f1' or other relevant metrics
    cv=tscv,
    verbose=2,
    random_state=42,
    n_jobs=-1  # Use all cores
)

# Fit the random search model
random_search_lgb.fit(X_train, y_train)

# Best hyperparameters
print("Best Hyperparameters:", random_search_lgb.best_params_)

# Train the model with best parameters on the full training set
best_lgb_model = lgb.LGBMClassifier(**random_search_lgb.best_params_)
best_lgb_model.fit(X_train, y_train)

# Predict on test set
y_pred_lgb = best_lgb_model.predict(X_test)

# Model Evaluation
accuracy_lgb = accuracy_score(y_test, y_pred_lgb)
f1_lgb = f1_score(y_test, y_pred_lgb, average='macro')  # Adjust based on class balance

print("Test Accuracy:", accuracy_lgb)
print("Test F1 Score:", f1_lgb)



--------------------------------

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking
from tensorflow.keras import utils
from sklearn.metrics import accuracy_score, f1_score

# Assuming df is your DataFrame
# and 'load_country_encoded' and 'discharge_country_encoded' are already label encoded

# Splitting the dataset into train, validation, and test sets
# Getting the last occurrence for each flight number for the test set
test_idx = df.groupby('no').tail(1).index
# Getting the second last occurrence for each flight number for the validation set
validation_idx = df.groupby('no').nth(-2).dropna().index
train_idx = df.index.difference(test_idx.union(validation_idx))

# Creating train, validation, and test sets
train = df.loc[train_idx]
validation = df.loc[validation_idx]
test = df.loc[test_idx]

# Creating sequences
def create_sequences(df, label_col):
    X_list, y_list = [], []
    for flight_no in df['no'].unique():
        flight_data = df[df['no'] == flight_no]
        X_list.append(flight_data.drop(['no', label_col], axis=1).values)
        y_list.append(flight_data[label_col].values[-1])
    return np.array(X_list), np.array(y_list)

X_train, y_train = create_sequences(train, 'load_country_encoded')
X_validation, y_validation = create_sequences(validation, 'load_country_encoded')
X_test, y_test = create_sequences(test, 'load_country_encoded')

# Padding the sequences
X_train_padded = utils.sequence.pad_sequences(X_train, padding='post', dtype='float32')
X_validation_padded = utils.sequence.pad_sequences(X_validation, padding='post', dtype='float32')
X_test_padded = utils.sequence.pad_sequences(X_test, padding='post', dtype='float32')

# Model parameters
max_features = df['load_country_encoded'].max() + 1  # Plus one for potential zero padding
embedding_size = 50  # Example embedding size, this is a hyperparameter you can tune
lstm_units = 50  # LSTM units, another hyperparameter

# Define LSTM model with Embedding
model = Sequential()
model.add(Embedding(input_dim=max_features, output_dim=embedding_size))
model.add(Masking(mask_value=0.))  # Masking layer for handling different sequence lengths
model.add(LSTM(lstm_units))
model.add(Dense(max_features, activation='softmax'))  # Output layer

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_padded, y_train, epochs=50, batch_size=32, 
                    validation_data=(X_validation_padded, y_validation), verbose=1)

# Prediction and evaluation
y_pred = model.predict(X_test_padded)
y_pred_classes = np.argmax(y_pred, axis=1)
print("Test Accuracy:", accuracy_score(y_test, y_pred_classes))
print("Test F1 Score:", f1_score(y_test, y_pred_classes, average='macro'))



------------------------------------------------------------------------

import pandas as pd

def update_top_locations(group):
    # Initialize a list to hold the top locations for each row
    top_locs = []
    # Initialize a dictionary to hold the frequency of each location
    freq = {}
    
    # Iterate over each load country in the group
    for country in group['load_en']:
        # Update the frequency dictionary
        freq[country] = freq.get(country, 0) + 1
        # Create a sorted list of countries by their frequency
        sorted_countries = sorted(freq, key=freq.get, reverse=True)
        # Append the top 5 countries to the top_locs list, using None if there are less than 5
        top_locs.append(sorted_countries[:5] + [None]*(5-len(sorted_countries)))
    
    # Convert the list of top locations into a DataFrame
    top_locs_df = pd.DataFrame(top_locs, columns=[f'top_loc_{i}' for i in range(1, 6)])
    # Shift the data down by one row so the current load country affects the next row
    group = group.join(top_locs_df.shift(1))
    return group

# Apply the function to each flight group
updated_df = data_df.groupby('flight_no').apply(update_top_locations).reset_index(drop=True)



------------- 

# Assuming df is your entire dataset and df_new is your unseen data
df_combined = pd.concat([df, df_new])

# Function to get the top countries for each flight
def get_top_countries(group):
    top_countries = group['load_country'].value_counts().index.tolist()
    next_idx = group.index[-1] + 1
    if next_idx in df_combined.index:  # Check if there's a next row
        for i in range(5):
            if i < len(top_countries):
                df_combined.at[next_idx, f'top_country_{i+1}'] = top_countries[i]
            else:
                df_combined.at[next_idx, f'top_country_{i+1}'] = None

df_combined.groupby('no').apply(get_top_countries)





---------------


import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras import backend as K

# Custom F1 score metric
def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# Prepare sequences
def create_sequences(X, y, unique_nos, take_last_n=1):
    X_list = []
    y_list = []
    for flight_no in unique_nos:
        flight_data = X[X['no'] == flight_no]
        X_list.append(flight_data.drop('no', axis=1).values)  # assuming 'no' is the flight number column
        y_list.append(y.loc[flight_data.index][-take_last_n])  # get only the last value for y
    return X_list, y_list

# Splitting the data into train and validation based on the last occurrence
unique_flight_numbers = X_train['no'].unique()
validation_flight_numbers = [X_train[X_train['no'] == no].iloc[-1] for no in unique_flight_numbers]
validation_indices = [index for index, _ in validation_flight_numbers]
X_val = X_train.loc[validation_indices]
y_val = y_train.loc[validation_indices]
X_train = X_train.drop(validation_indices)
y_train = y_train.drop(validation_indices)

# Creating sequences for training, validation, and test sets
X_train_list, y_train_list = create_sequences(X_train, y_train, X_train['no'].unique())
X_val_list, y_val_list = create_sequences(X_val, y_val, X_val['no'].unique())
X_test_list, y_test_list = create_sequences(X_test, y_test, X_test['no'].unique())

# Padding sequences
X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_list, padding='post', dtype='float32')
y_train_padded = np.array(y_train_list)
X_val_padded = tf.keras.preprocessing.sequence.pad_sequences(X_val_list, padding='post', dtype='float32')
y_val_padded = np.array(y_val_list)
X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_list, padding='post', dtype='float32')
y_test_padded = np.array(y_test_list)

# Define LSTM model
model = Sequential()
model.add(Masking(mask_value=0., input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])))  # Masking layer for padded values
model.add(LSTM(50, return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(50))
model.add(Dropout(0.1))
model.add(Dense(len(np.unique(y_train)), activation='softmax'))

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[f1])

# Train model
history = model.fit(X_train_padded, y_train_padded, epochs=50, batch_size=32, verbose=1,
                    validation_data=(X_val_padded, y_val_padded), callbacks=[early_stopping])

# Prediction and Evaluation on test set
y_pred = model.predict(X_test_padded)
y_pred_classes = np.argmax(y_pred, axis=1)

print("Accuracy on test set:", accuracy_score(y_test_padded, y_pred_classes))
print("F1 Score on test set:", f1_score(y_test_padded, y_pred_classes, average='macro'))





