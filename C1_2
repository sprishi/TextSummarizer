import pandas as pd

# Load the data from Dataiku
df = pd.read_csv('path_to_your_dataframe.csv')

# Rename columns
df.rename(columns={
    'Date': 'Model_Date',
    'Country': 'Load_Region',
    'Covid Positive or Negative': 'Hit_Fail_Status'
}, inplace=True)

# Aggregate data
agg_df = df.groupby(['Model_Date', 'Load_Region', 'Hit_Fail_Status']).size().unstack(fill_value=0).reset_index()

# Calculate Total counts
agg_df['Total'] = agg_df['Positive'] + agg_df['Negative']

# Save the aggregated data back to Dataiku
agg_df.to_csv('aggregated_data.csv', index=False)





---------

from bokeh.io import curdoc
from bokeh.models import ColumnDataSource, DataTable, TableColumn, DateFormatter, Dropdown, Slider
from bokeh.layouts import layout, widgetbox
from bokeh.plotting import figure
import pandas as pd

# Load the aggregated data
agg_df = pd.read_csv('path_to_your_aggregated_dataframe.csv')

# Create a ColumnDataSource from the DataFrame
source = ColumnDataSource(agg_df)

# Create DataTable columns
columns = [
    TableColumn(field="Model_Date", title="Model_Date", formatter=DateFormatter()),
    TableColumn(field="Load_Region", title="Load_Region"),
    TableColumn(field="Total", title="Total"),
    TableColumn(field="Positive", title="Positive"),
    TableColumn(field="Negative", title="Negative")
]

# Create a DataTable
data_table = DataTable(source=source, columns=columns, width=800, height=280)

# Create a dropdown filter for countries
country_options = [(region, region) for region in agg_df['Load_Region'].unique()]
dropdown = Dropdown(label="Select Country", button_type="warning", menu=country_options)

# Create a date slider filter
date_slider = Slider(title="Date Range", start=agg_df['Model_Date'].min(), end=agg_df['Model_Date'].max(), step=1)

# Define callback function for filtering
def update():
    selected_country = dropdown.value
    selected_date = date_slider.value

    filtered_df = agg_df[(agg_df['Load_Region'] == selected_country) & (agg_df['Model_Date'] <= selected_date)]
    source.data = filtered_df

dropdown.on_change('value', lambda attr, old, new: update())
date_slider.on_change('value', lambda attr, old, new: update())

# Layout the widgets and table
layout = layout([
    [widgetbox(dropdown, date_slider)],
    [data_table]
])

curdoc().add_root(layout)
curdoc().title = "Covid-19 Data Visualization"







---------

---------

import pandas as pd
import math
from shapely.geometry import Point, Polygon

# Define your polygon points
polygon_points = [(lat1, lon1), (lat2, lon2), (lat3, lon3), (lat4, lon4)]
polygon = Polygon(polygon_points)

# Sample DataFrame
data = {
    'IMO': ['vessel1', 'vessel1', 'vessel1', 'vessel2', 'vessel2'],
    'Latitude': [lat1, lat2, lat3, lat1, lat2],
    'Longitude': [lon1, lon2, lon3, lon1, lon2],
    'Heading': [45, 90, 135, 180, 225]
}
df = pd.DataFrame(data)

# Function to determine movement status
def is_heading_towards_polygon(current_heading, polygon_centroid, current_position):
    delta_y = polygon_centroid.y - current_position[0]
    delta_x = polygon_centroid.x - current_position[1]
    angle_to_polygon = math.degrees(math.atan2(delta_y, delta_x))
    
    # Normalize angles to the range [0, 360)
    angle_to_polygon = angle_to_polygon % 360
    current_heading = current_heading % 360
    
    # Allow for a margin of error (e.g., 15 degrees) to account for slight deviations
    margin = 15
    return abs(current_heading - angle_to_polygon) <= margin or abs(current_heading - angle_to_polygon) >= (360 - margin)

def determine_movement_status(group, polygon):
    history = []
    statuses = []
    for _, row in group.iterrows():
        current_position = (row['Latitude'], row['Longitude'])
        current_heading = row['Heading']
        history.append((current_position, current_heading))
        
        if len(history) < 2:
            statuses.append("Unknown")
            continue

        previous_position, previous_heading = history[-2]
        prev_in_polygon = polygon.contains(Point(previous_position))
        curr_in_polygon = polygon.contains(Point(current_position))
        heading_towards_polygon = is_heading_towards_polygon(current_heading, polygon.centroid, current_position)

        if not prev_in_polygon and curr_in_polygon and heading_towards_polygon:
            statuses.append("Entering")
        elif prev_in_polygon and not curr_in_polygon and not heading_towards_polygon:
            statuses.append("Exiting")
        else:
            statuses.append("Inside" if curr_in_polygon else "Outside")

    return statuses

# Apply the function to each group
df['Status'] = df.groupby('IMO').apply(lambda x: determine_movement_status(x, polygon)).explode().values

# Display the DataFrame with the new Status column
print(df)


///////


import math
from shapely.geometry import Point, Polygon

# Step 1: Track Vessel History with Heading
vessel_history = {}

def update_vessel_history(vessel_id, position, heading):
    if vessel_id not in vessel_history:
        vessel_history[vessel_id] = []
    vessel_history[vessel_id].append((position, heading))
    if len(vessel_history[vessel_id]) > 3:
        vessel_history[vessel_id].pop(0)  # Keep only the last 3 positions and headings

# Step 2: Calculate Movement Direction
def is_heading_towards_polygon(current_heading, polygon_centroid, current_position):
    # Calculate the direction from the current position to the polygon centroid
    delta_y = polygon_centroid.y - current_position[0]
    delta_x = polygon_centroid.x - current_position[1]
    angle_to_polygon = math.degrees(math.atan2(delta_y, delta_x))
    
    # Normalize angles to the range [0, 360)
    angle_to_polygon = angle_to_polygon % 360
    current_heading = current_heading % 360
    
    # Allow for a margin of error (e.g., 15 degrees) to account for slight deviations
    margin = 15
    return abs(current_heading - angle_to_polygon) <= margin or abs(current_heading - angle_to_polygon) >= (360 - margin)

# Step 3: Determine Entering or Exiting with Heading
def is_entering_or_exiting(vessel_id, current_position, current_heading, polygon):
    if vessel_id not in vessel_history:
        return "Unknown"

    history = vessel_history[vessel_id]
    if len(history) < 2:
        return "Unknown"

    previous_position, previous_heading = history[-2]
    prev_in_polygon = polygon.contains(Point(previous_position))
    curr_in_polygon = polygon.contains(Point(current_position))

    heading_towards_polygon = is_heading_towards_polygon(current_heading, polygon.centroid, current_position)

    if not prev_in_polygon and curr_in_polygon and heading_towards_polygon:
        return "Entering"
    elif prev_in_polygon and not curr_in_polygon and not heading_towards_polygon:
        return "Exiting"

    return "Inside" if curr_in_polygon else "Outside"

# Step 4: Update Your Alert Logic
def check_vessel(vessel_id, current_position, current_heading, polygon):
    update_vessel_history(vessel_id, current_position, current_heading)
    movement_status = is_entering_or_exiting(vessel_id, current_position, current_heading, polygon)
    
    if movement_status == "Entering":
        send_alert(vessel_id, current_position, "Entering")
    elif movement_status == "Exiting":
        send_alert(vessel_id, current_position, "Exiting")

def send_alert(vessel_id, position, status):
    # Implement your alerting logic here (e.g., send an email)
    print(f"Alert: Vessel {vessel_id} is {status} at position {position}")

# Example Usage
# Define your polygon points
polygon_points = [(lat1, lon1), (lat2, lon2), (lat3, lon3), (lat4, lon4)]
polygon = Polygon(polygon_points)

# Update and check vessel status
vessel_id = "vessel123"
current_position = (lat, lon)
current_heading = 45  # Example heading in degrees

check_vessel(vessel_id, current_position, current_heading, polygon)



--------------

import pandas as pd

# Assuming your DataFrame is called 'df'
# df = ... (load your DataFrame)

# Get the list of unique regions
regions = df['Load_Region'].unique()

# Create a MultiIndex for the rows
row_index = pd.MultiIndex.from_product([['Total', 'Hit', 'Fail'], df['Model_Date'].unique()], names=['Metric', None])

# Create a new DataFrame with the desired structure
result = pd.DataFrame(index=row_index, columns=['Model_Date'] + ['Count'] + list(regions))
result = result.sort_index(level=1, ascending=False)  # Sort rows by Model_Date in descending order

# Group the data by Model_Date and Load_Region
grouped = df.groupby(['Model_Date', 'Load_Region'])

# Populate the result DataFrame
for (model_date, region), group in grouped:
    total_count = len(group)
    hit_count = (group['Hit_Fail_Status'] == 'Hit').sum()
    fail_count = (group['Hit_Fail_Status'] == 'Fail').sum()
    
    result.loc[('Total', model_date), 'Model_Date'] = model_date
    result.loc[('Hit', model_date), 'Model_Date'] = model_date
    result.loc[('Fail', model_date), 'Model_Date'] = model_date
    
    result.loc[('Total', model_date), 'Count'] = 'Total'
    result.loc[('Hit', model_date), 'Count'] = 'Hit'
    result.loc[('Fail', model_date), 'Count'] = 'Fail'
    
    result.loc[('Total', model_date), region] = total_count
    result.loc[('Hit', model_date), region] = hit_count
    result.loc[('Fail', model_date), region] = fail_count

# Sort the columns by the Total count in descending order
result = result.reindex(sorted(result.columns[2:], key=lambda x: result.loc[('Total'), x].sum(), reverse=True), level=0, axis=1)

print(result)






def process_batch(ais_batch, ports_data):
    # Cross join - simple implementation for demonstration
    joined = ais_batch.assign(key=1).merge(ports_data.assign(key=1), on='key').drop('key', 1)
    
    # Calculate distances
    joined['distance'] = joined.apply(lambda row: haversine(row['ais_lon'], row['ais_lat'], row['port_lon'], row['port_lat']), axis=1)
    
    # Keep only the closest port for each AIS record
    closest = joined.loc[joined.groupby('ais_id')['distance'].idxmin()]
    
    return closest

def process_in_batches(ais_data, ports_data, batch_size=1000):
    # Splitting AIS data into batches
    batches = [ais_data.iloc[i:i + batch_size] for i in range(0, ais_data.shape[0], batch_size)]
    
    results = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(process_batch, batch, ports_data) for batch in batches]
        for future in as_completed(futures):
            results.append(future.result())
    
    # Combine results from all batches
    final_result = pd.concat(results, ignore_index=True)
    return final_result

# Assuming ais_data and ports_data are already loaded
# final_result_df = process_in_batches(ais_data, ports_data)



-------------------

Sure, I'll provide you with the complete implementation of the `segment_trajectories` function and an optimized code for training the model efficiently on large datasets using PySpark.

First, let's define the `segment_trajectories` function:

```python
from pyspark.sql.functions import *
from pyspark.sql.window import Window

@udf("array<struct<lat:double,lon:double,timestamp:timestamp>>")
def segment_trajectories(trajectory):
    """
    Function to segment a vessel's trajectory based on spatial and temporal proximity.
    
    Args:
        trajectory (pyspark.sql.DataFrame): A DataFrame containing the vessel's trajectory data.
        
    Returns:
        list: A list of trajectory segments, where each segment is a list of (lat, lon, timestamp) tuples.
    """
    segments = []
    current_segment = []
    prev_lat, prev_lon, prev_ts = None, None, None
    
    # Sort the trajectory by timestamp
    sorted_trajectory = trajectory.sort("timestamp").collect()
    
    for row in sorted_trajectory:
        lat, lon, ts = row.lat, row.lon, row.timestamp
        
        # Check if the current point is within spatial and temporal proximity of the previous point
        if prev_lat is not None and (haversine_distance(lat, lon, prev_lat, prev_lon) > spatial_threshold or (ts - prev_ts).total_seconds() > temporal_threshold):
            # If not, start a new segment
            if current_segment:
                segments.append(current_segment)
                current_segment = []
        
        # Add the current point to the current segment
        current_segment.append((lat, lon, ts))
        prev_lat, prev_lon, prev_ts = lat, lon, ts
    
    # Add the last segment
    if current_segment:
        segments.append(current_segment)
    
    return segments
```

This function takes a DataFrame containing a vessel's trajectory data (latitude, longitude, and timestamp) and segments the trajectory based on spatial and temporal proximity thresholds. The spatial threshold (`spatial_threshold`) determines the maximum distance between consecutive points to be considered part of the same segment, while the temporal threshold (`temporal_threshold`) determines the maximum time difference.

The function returns a list of trajectory segments, where each segment is a list of tuples containing (latitude, longitude, timestamp). You can adjust the `spatial_threshold` and `temporal_threshold` values based on your specific requirements.

Now, for the optimized code to train the model efficiently on large datasets using PySpark:

```python
from pyspark.sql.functions import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
from collections import Counter

# Load AIS and port datasets
ais_data = spark.table("ais_data")
ports_data = spark.table("ports_data")

# User-defined functions
@udf("double")
def haversine_distance(lat1, lon1, lat2, lon2):
    # Haversine distance calculation (same as before)
    ...

@udf("double")
def bearing(lat1, lon1, lat2, lon2):
    # Bearing calculation (same as before)
    ...

# Data preprocessing and cleaning
cleaned_ais_data = ais_data.na.drop() \
                            .withColumn("timestamp", to_timestamp("timestamp_str", "yyyy-MM-dd HH:mm:ss")) \
                            .dropDuplicates()

# Join AIS data with port data
ais_with_ports = cleaned_ais_data.join(ports_data, [haversine_distance(ais_data.lat, ais_data.lon, ports_data.lat, ports_data.lon) < distance_threshold], "left") \
                                 .select(cleaned_ais_data.columns + ["port_name", "port_lat", "port_lon"])

# Feature engineering
feature_engineered_data = ais_with_ports.withColumn("dist_to_port", haversine_distance(ais_with_ports.lat, ais_with_ports.lon, ais_with_ports.port_lat, ais_with_ports.port_lon)) \
                                         .withColumn("bearing_to_port", bearing(ais_with_ports.lat, ais_with_ports.lon, ais_with_ports.port_lat, ais_with_ports.port_lon)) \
                                         .withColumn("hour", hour("timestamp")) \
                                         .withColumn("day_of_week", dayofweek("timestamp")) \
                                         .withColumn("speed_change", lag("speed").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "speed") \
                                         .withColumn("heading_change", lag("heading").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "heading")

# Identify trajectory segments
trajectory_segments = feature_engineered_data.groupBy("vessel_id") \
                                              .flatMap(lambda x: segment_trajectories(x[1])) \
                                              .toDF(["segment_id", "vessel_id", "trajectory_segment"])

# Vectorize trajectory segments
vector_assembler = VectorAssembler(inputCols=["dist_to_port", "bearing_to_port", "hour", "day_of_week", "speed_change", "heading_change"],
                                   outputCol="features")
vectorized_segments = vector_assembler.transform(trajectory_segments)

# Trajectory clustering
kmeans = KMeans(featuresCol="features", k=10, seed=42)
pipeline = Pipeline(stages=[kmeans])

# Train the model using PySpark's model persistence for efficient training
cluster_model = pipeline.fit(vectorized_segments.repartition(200).cache())

# Save the trained model
cluster_model.write().overwrite().save("path/to/model")

# Function to estimate docking probabilities for a given cluster and trajectory segment
def estimate_docking_probabilities(cluster_id, trajectory_segment):
    # Filter historical segments in the same cluster
    cluster_segments = clustered_segments.filter(clustered_segments.prediction == cluster_id)
    
    # Extract the port names from the cluster segments
    port_names = cluster_segments.select("port_name").rdd.flatMap(lambda x: x).collect()
    
    # Count the occurrences of each port name
    port_counts = Counter(port_names)
    total_counts = sum(port_counts.values())
    
    # Calculate the docking probabilities
    docking_probabilities = {port: count / total_counts for port, count in port_counts.items()}
    
    return docking_probabilities

# Load the trained model
cluster_model = KMeansModel.load("path/to/model")

# Preprocess and feature engineer current data
current_ais_data = spark.table("current_ais_data") # Load current AIS data
current_feature_engineered = current_ais_data.withColumn("dist_to_port", haversine_distance(current_ais_data.lat, current_ais_data.lon, ports_data.lat, ports_data.lon)) \
                                              .withColumn("bearing_to_port", bearing(current_ais_data.lat, current_ais_data.lon, ports_data.lat, ports_data.lon)) \
                                              .withColumn("hour", hour("timestamp")) \
                                              .withColumn("day_of_week", dayofweek("timestamp")) \
                                              .withColumn("speed_change", lag("speed").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "speed") \
                                              .withColumn("heading_change", lag("heading").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "heading")

# Vectorize current trajectory segments
vectorized_current_segments = vector_assembler.transform(current_feature_engineered)

# Predict cluster assignment for current segments
current_segment_clusters = cluster_model.transform(vectorized_current_segments)

# Estimate docking probabilities for current segments
docking_predictions = current_segment_clusters.rdd.map(lambda row: (row.segment_id, estimate_docking_probabilities(row.prediction, row.trajectory_segment))).toDF(["segment_id", "
