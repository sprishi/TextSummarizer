


===============================

import pandas as pd
import numpy as np
import pmdarima as pm
import matplotlib.pyplot as plt

# Step 1: Read and preprocess the data
file_path = '/mnt/data/Weekly_U.S._Exports_of_Crude_Oil.csv'
data = pd.read_csv(file_path, skiprows=4)

# Assuming the headers start from the fifth row
data.columns = data.iloc[0]
data = data[1:]
data.columns = ['Week Ending', 'Volume (KBD)']
data['Week Ending'] = pd.to_datetime(data['Week Ending'])
data['Volume (KBD)'] = data['Volume (KBD)'].astype(float)

# Step 2: Transform weekly data to daily and then to monthly data
data['Volume (Thousand Barrels)'] = data['Volume (KBD)'] * 7

# Group by month and sum the volumes
data.set_index('Week Ending', inplace=True)

# Create a daily time series by forward filling the weekly data
daily_data = data.resample('D').ffill()

# Aggregate to monthly data by summing
monthly_data = daily_data.resample('M').sum()

# Step 3: Train the ARIMA model
# Use data from January 2019 to April 2024
start_date = '2019-01-01'
end_date = '2024-04-30'
train_data = monthly_data[start_date:end_date]

# Fit the auto_arima model
model = pm.auto_arima(train_data['Volume (Thousand Barrels)'], seasonal=True, m=12)

# Step 4: Forecast for the next four months
forecast_periods = 4
forecast = model.predict(n_periods=forecast_periods)

# Prepare forecast data for plotting
forecast_dates = pd.date_range(start=train_data.index[-1] + pd.offsets.MonthBegin(), periods=forecast_periods, freq='M')
forecast_series = pd.Series(forecast, index=forecast_dates)

# Plotting the results
plt.figure(figsize=(14, 7))
plt.plot(train_data.index, train_data['Volume (Thousand Barrels)'], label='Historical Data')
plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red')
plt.xlabel('Date')
plt.ylabel('Volume (Thousand Barrels)')
plt.title('US Crude Oil Export Forecast')
plt.legend()
plt.show()

# Display forecast values
print("Forecasted values for the next 4 months:")
print(forecast_series)



---------

import pandas as pd
from pmdarima import auto_arima

# Read the data from the CSV file
data = pd.read_csv('Weekly_U.S._Exports_of_Crude_Oil.csv', header=4)

# Convert the 'Week of' column to datetime format
data['Week of'] = pd.to_datetime(data['Week of'])

# Set the 'Week of' column as the index
data.set_index('Week of', inplace=True)

# Multiply the weekly values by 7 to get the total volume for the week
data['Weekly U.S. Exports of Crude Oil Thousand Barrels per Day'] *= 7

# Resample the data to monthly frequency and sum the values
monthly_data = data['Weekly U.S. Exports of Crude Oil Thousand Barrels per Day'].resample('M').sum()

# Convert the monthly data back to a DataFrame
monthly_data = monthly_data.to_frame().reset_index()

# Rename the columns
monthly_data.columns = ['Month', 'Exports']

# Convert the 'Exports' column to numeric type
monthly_data['Exports'] = monthly_data['Exports'].astype(float)

# Split the data into training and test sets
train_data = monthly_data[monthly_data['Month'] < pd.to_datetime('2024-05-01')]
test_data = monthly_data[monthly_data['Month'] >= pd.to_datetime('2024-05-01')]

# Train the ARIMA model
model = auto_arima(train_data['Exports'], seasonal=True, m=12, suppress_warnings=True)

# Forecast the next 4 months
forecast = model.predict(n_periods=4)

# Print the forecast
print(forecast)



---------

import pandas as pd
import math
from shapely.geometry import Point, Polygon

# Define your polygon points
polygon_points = [(lat1, lon1), (lat2, lon2), (lat3, lon3), (lat4, lon4)]
polygon = Polygon(polygon_points)

# Sample DataFrame
data = {
    'IMO': ['vessel1', 'vessel1', 'vessel1', 'vessel2', 'vessel2'],
    'Latitude': [lat1, lat2, lat3, lat1, lat2],
    'Longitude': [lon1, lon2, lon3, lon1, lon2],
    'Heading': [45, 90, 135, 180, 225]
}
df = pd.DataFrame(data)

# Function to determine movement status
def is_heading_towards_polygon(current_heading, polygon_centroid, current_position):
    delta_y = polygon_centroid.y - current_position[0]
    delta_x = polygon_centroid.x - current_position[1]
    angle_to_polygon = math.degrees(math.atan2(delta_y, delta_x))
    
    # Normalize angles to the range [0, 360)
    angle_to_polygon = angle_to_polygon % 360
    current_heading = current_heading % 360
    
    # Allow for a margin of error (e.g., 15 degrees) to account for slight deviations
    margin = 15
    return abs(current_heading - angle_to_polygon) <= margin or abs(current_heading - angle_to_polygon) >= (360 - margin)

def determine_movement_status(group, polygon):
    history = []
    statuses = []
    for _, row in group.iterrows():
        current_position = (row['Latitude'], row['Longitude'])
        current_heading = row['Heading']
        history.append((current_position, current_heading))
        
        if len(history) < 2:
            statuses.append("Unknown")
            continue

        previous_position, previous_heading = history[-2]
        prev_in_polygon = polygon.contains(Point(previous_position))
        curr_in_polygon = polygon.contains(Point(current_position))
        heading_towards_polygon = is_heading_towards_polygon(current_heading, polygon.centroid, current_position)

        if not prev_in_polygon and curr_in_polygon and heading_towards_polygon:
            statuses.append("Entering")
        elif prev_in_polygon and not curr_in_polygon and not heading_towards_polygon:
            statuses.append("Exiting")
        else:
            statuses.append("Inside" if curr_in_polygon else "Outside")

    return statuses

# Apply the function to each group
df['Status'] = df.groupby('IMO').apply(lambda x: determine_movement_status(x, polygon)).explode().values

# Display the DataFrame with the new Status column
print(df)


///////


import math
from shapely.geometry import Point, Polygon

# Step 1: Track Vessel History with Heading
vessel_history = {}

def update_vessel_history(vessel_id, position, heading):
    if vessel_id not in vessel_history:
        vessel_history[vessel_id] = []
    vessel_history[vessel_id].append((position, heading))
    if len(vessel_history[vessel_id]) > 3:
        vessel_history[vessel_id].pop(0)  # Keep only the last 3 positions and headings

# Step 2: Calculate Movement Direction
def is_heading_towards_polygon(current_heading, polygon_centroid, current_position):
    # Calculate the direction from the current position to the polygon centroid
    delta_y = polygon_centroid.y - current_position[0]
    delta_x = polygon_centroid.x - current_position[1]
    angle_to_polygon = math.degrees(math.atan2(delta_y, delta_x))
    
    # Normalize angles to the range [0, 360)
    angle_to_polygon = angle_to_polygon % 360
    current_heading = current_heading % 360
    
    # Allow for a margin of error (e.g., 15 degrees) to account for slight deviations
    margin = 15
    return abs(current_heading - angle_to_polygon) <= margin or abs(current_heading - angle_to_polygon) >= (360 - margin)

# Step 3: Determine Entering or Exiting with Heading
def is_entering_or_exiting(vessel_id, current_position, current_heading, polygon):
    if vessel_id not in vessel_history:
        return "Unknown"

    history = vessel_history[vessel_id]
    if len(history) < 2:
        return "Unknown"

    previous_position, previous_heading = history[-2]
    prev_in_polygon = polygon.contains(Point(previous_position))
    curr_in_polygon = polygon.contains(Point(current_position))

    heading_towards_polygon = is_heading_towards_polygon(current_heading, polygon.centroid, current_position)

    if not prev_in_polygon and curr_in_polygon and heading_towards_polygon:
        return "Entering"
    elif prev_in_polygon and not curr_in_polygon and not heading_towards_polygon:
        return "Exiting"

    return "Inside" if curr_in_polygon else "Outside"

# Step 4: Update Your Alert Logic
def check_vessel(vessel_id, current_position, current_heading, polygon):
    update_vessel_history(vessel_id, current_position, current_heading)
    movement_status = is_entering_or_exiting(vessel_id, current_position, current_heading, polygon)
    
    if movement_status == "Entering":
        send_alert(vessel_id, current_position, "Entering")
    elif movement_status == "Exiting":
        send_alert(vessel_id, current_position, "Exiting")

def send_alert(vessel_id, position, status):
    # Implement your alerting logic here (e.g., send an email)
    print(f"Alert: Vessel {vessel_id} is {status} at position {position}")

# Example Usage
# Define your polygon points
polygon_points = [(lat1, lon1), (lat2, lon2), (lat3, lon3), (lat4, lon4)]
polygon = Polygon(polygon_points)

# Update and check vessel status
vessel_id = "vessel123"
current_position = (lat, lon)
current_heading = 45  # Example heading in degrees

check_vessel(vessel_id, current_position, current_heading, polygon)



--------------

import pandas as pd

# Assuming your DataFrame is called 'df'
# df = ... (load your DataFrame)

# Get the list of unique regions
regions = df['Load_Region'].unique()

# Create a MultiIndex for the rows
row_index = pd.MultiIndex.from_product([['Total', 'Hit', 'Fail'], df['Model_Date'].unique()], names=['Metric', None])

# Create a new DataFrame with the desired structure
result = pd.DataFrame(index=row_index, columns=['Model_Date'] + ['Count'] + list(regions))
result = result.sort_index(level=1, ascending=False)  # Sort rows by Model_Date in descending order

# Group the data by Model_Date and Load_Region
grouped = df.groupby(['Model_Date', 'Load_Region'])

# Populate the result DataFrame
for (model_date, region), group in grouped:
    total_count = len(group)
    hit_count = (group['Hit_Fail_Status'] == 'Hit').sum()
    fail_count = (group['Hit_Fail_Status'] == 'Fail').sum()
    
    result.loc[('Total', model_date), 'Model_Date'] = model_date
    result.loc[('Hit', model_date), 'Model_Date'] = model_date
    result.loc[('Fail', model_date), 'Model_Date'] = model_date
    
    result.loc[('Total', model_date), 'Count'] = 'Total'
    result.loc[('Hit', model_date), 'Count'] = 'Hit'
    result.loc[('Fail', model_date), 'Count'] = 'Fail'
    
    result.loc[('Total', model_date), region] = total_count
    result.loc[('Hit', model_date), region] = hit_count
    result.loc[('Fail', model_date), region] = fail_count

# Sort the columns by the Total count in descending order
result = result.reindex(sorted(result.columns[2:], key=lambda x: result.loc[('Total'), x].sum(), reverse=True), level=0, axis=1)

print(result)






def process_batch(ais_batch, ports_data):
    # Cross join - simple implementation for demonstration
    joined = ais_batch.assign(key=1).merge(ports_data.assign(key=1), on='key').drop('key', 1)
    
    # Calculate distances
    joined['distance'] = joined.apply(lambda row: haversine(row['ais_lon'], row['ais_lat'], row['port_lon'], row['port_lat']), axis=1)
    
    # Keep only the closest port for each AIS record
    closest = joined.loc[joined.groupby('ais_id')['distance'].idxmin()]
    
    return closest

def process_in_batches(ais_data, ports_data, batch_size=1000):
    # Splitting AIS data into batches
    batches = [ais_data.iloc[i:i + batch_size] for i in range(0, ais_data.shape[0], batch_size)]
    
    results = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(process_batch, batch, ports_data) for batch in batches]
        for future in as_completed(futures):
            results.append(future.result())
    
    # Combine results from all batches
    final_result = pd.concat(results, ignore_index=True)
    return final_result

# Assuming ais_data and ports_data are already loaded
# final_result_df = process_in_batches(ais_data, ports_data)



-------------------

Sure, I'll provide you with the complete implementation of the `segment_trajectories` function and an optimized code for training the model efficiently on large datasets using PySpark.

First, let's define the `segment_trajectories` function:

```python
from pyspark.sql.functions import *
from pyspark.sql.window import Window

@udf("array<struct<lat:double,lon:double,timestamp:timestamp>>")
def segment_trajectories(trajectory):
    """
    Function to segment a vessel's trajectory based on spatial and temporal proximity.
    
    Args:
        trajectory (pyspark.sql.DataFrame): A DataFrame containing the vessel's trajectory data.
        
    Returns:
        list: A list of trajectory segments, where each segment is a list of (lat, lon, timestamp) tuples.
    """
    segments = []
    current_segment = []
    prev_lat, prev_lon, prev_ts = None, None, None
    
    # Sort the trajectory by timestamp
    sorted_trajectory = trajectory.sort("timestamp").collect()
    
    for row in sorted_trajectory:
        lat, lon, ts = row.lat, row.lon, row.timestamp
        
        # Check if the current point is within spatial and temporal proximity of the previous point
        if prev_lat is not None and (haversine_distance(lat, lon, prev_lat, prev_lon) > spatial_threshold or (ts - prev_ts).total_seconds() > temporal_threshold):
            # If not, start a new segment
            if current_segment:
                segments.append(current_segment)
                current_segment = []
        
        # Add the current point to the current segment
        current_segment.append((lat, lon, ts))
        prev_lat, prev_lon, prev_ts = lat, lon, ts
    
    # Add the last segment
    if current_segment:
        segments.append(current_segment)
    
    return segments
```

This function takes a DataFrame containing a vessel's trajectory data (latitude, longitude, and timestamp) and segments the trajectory based on spatial and temporal proximity thresholds. The spatial threshold (`spatial_threshold`) determines the maximum distance between consecutive points to be considered part of the same segment, while the temporal threshold (`temporal_threshold`) determines the maximum time difference.

The function returns a list of trajectory segments, where each segment is a list of tuples containing (latitude, longitude, timestamp). You can adjust the `spatial_threshold` and `temporal_threshold` values based on your specific requirements.

Now, for the optimized code to train the model efficiently on large datasets using PySpark:

```python
from pyspark.sql.functions import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
from collections import Counter

# Load AIS and port datasets
ais_data = spark.table("ais_data")
ports_data = spark.table("ports_data")

# User-defined functions
@udf("double")
def haversine_distance(lat1, lon1, lat2, lon2):
    # Haversine distance calculation (same as before)
    ...

@udf("double")
def bearing(lat1, lon1, lat2, lon2):
    # Bearing calculation (same as before)
    ...

# Data preprocessing and cleaning
cleaned_ais_data = ais_data.na.drop() \
                            .withColumn("timestamp", to_timestamp("timestamp_str", "yyyy-MM-dd HH:mm:ss")) \
                            .dropDuplicates()

# Join AIS data with port data
ais_with_ports = cleaned_ais_data.join(ports_data, [haversine_distance(ais_data.lat, ais_data.lon, ports_data.lat, ports_data.lon) < distance_threshold], "left") \
                                 .select(cleaned_ais_data.columns + ["port_name", "port_lat", "port_lon"])

# Feature engineering
feature_engineered_data = ais_with_ports.withColumn("dist_to_port", haversine_distance(ais_with_ports.lat, ais_with_ports.lon, ais_with_ports.port_lat, ais_with_ports.port_lon)) \
                                         .withColumn("bearing_to_port", bearing(ais_with_ports.lat, ais_with_ports.lon, ais_with_ports.port_lat, ais_with_ports.port_lon)) \
                                         .withColumn("hour", hour("timestamp")) \
                                         .withColumn("day_of_week", dayofweek("timestamp")) \
                                         .withColumn("speed_change", lag("speed").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "speed") \
                                         .withColumn("heading_change", lag("heading").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "heading")

# Identify trajectory segments
trajectory_segments = feature_engineered_data.groupBy("vessel_id") \
                                              .flatMap(lambda x: segment_trajectories(x[1])) \
                                              .toDF(["segment_id", "vessel_id", "trajectory_segment"])

# Vectorize trajectory segments
vector_assembler = VectorAssembler(inputCols=["dist_to_port", "bearing_to_port", "hour", "day_of_week", "speed_change", "heading_change"],
                                   outputCol="features")
vectorized_segments = vector_assembler.transform(trajectory_segments)

# Trajectory clustering
kmeans = KMeans(featuresCol="features", k=10, seed=42)
pipeline = Pipeline(stages=[kmeans])

# Train the model using PySpark's model persistence for efficient training
cluster_model = pipeline.fit(vectorized_segments.repartition(200).cache())

# Save the trained model
cluster_model.write().overwrite().save("path/to/model")

# Function to estimate docking probabilities for a given cluster and trajectory segment
def estimate_docking_probabilities(cluster_id, trajectory_segment):
    # Filter historical segments in the same cluster
    cluster_segments = clustered_segments.filter(clustered_segments.prediction == cluster_id)
    
    # Extract the port names from the cluster segments
    port_names = cluster_segments.select("port_name").rdd.flatMap(lambda x: x).collect()
    
    # Count the occurrences of each port name
    port_counts = Counter(port_names)
    total_counts = sum(port_counts.values())
    
    # Calculate the docking probabilities
    docking_probabilities = {port: count / total_counts for port, count in port_counts.items()}
    
    return docking_probabilities

# Load the trained model
cluster_model = KMeansModel.load("path/to/model")

# Preprocess and feature engineer current data
current_ais_data = spark.table("current_ais_data") # Load current AIS data
current_feature_engineered = current_ais_data.withColumn("dist_to_port", haversine_distance(current_ais_data.lat, current_ais_data.lon, ports_data.lat, ports_data.lon)) \
                                              .withColumn("bearing_to_port", bearing(current_ais_data.lat, current_ais_data.lon, ports_data.lat, ports_data.lon)) \
                                              .withColumn("hour", hour("timestamp")) \
                                              .withColumn("day_of_week", dayofweek("timestamp")) \
                                              .withColumn("speed_change", lag("speed").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "speed") \
                                              .withColumn("heading_change", lag("heading").over(Window.partitionBy("vessel_id").orderBy("timestamp")) - "heading")

# Vectorize current trajectory segments
vectorized_current_segments = vector_assembler.transform(current_feature_engineered)

# Predict cluster assignment for current segments
current_segment_clusters = cluster_model.transform(vectorized_current_segments)

# Estimate docking probabilities for current segments
docking_predictions = current_segment_clusters.rdd.map(lambda row: (row.segment_id, estimate_docking_probabilities(row.prediction, row.trajectory_segment))).toDF(["segment_id", "
